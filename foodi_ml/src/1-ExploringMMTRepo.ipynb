{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6805e262",
   "metadata": {},
   "source": [
    "# Exploration on MMT Retrieval repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "293f841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HOME_DIR = \"/home/ec2-user/SageMaker\"\n",
    "os.chdir(f\"{HOME_DIR}/MMT-Retrieval/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9d0b047",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence_transformers>=0.4.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (2.0.0)\n",
      "Requirement already satisfied: tqdm>=4.32.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (4.62.0)\n",
      "Requirement already satisfied: requests>=2.22.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (2.25.1)\n",
      "Requirement already satisfied: transformers>=4.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (4.9.1)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from -r requirements.txt (line 5)) (1.19.5)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from -r requirements.txt (line 6)) (1.9.0)\n",
      "Requirement already satisfied: huggingface-hub in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sentence_transformers>=0.4.1.2->-r requirements.txt (line 1)) (0.0.12)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sentence_transformers>=0.4.1.2->-r requirements.txt (line 1)) (0.10.0)\n",
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sentence_transformers>=0.4.1.2->-r requirements.txt (line 1)) (3.4.4)\n",
      "Requirement already satisfied: sentencepiece in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sentence_transformers>=0.4.1.2->-r requirements.txt (line 1)) (0.1.96)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sentence_transformers>=0.4.1.2->-r requirements.txt (line 1)) (1.5.3)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sentence_transformers>=0.4.1.2->-r requirements.txt (line 1)) (0.24.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.22.0->-r requirements.txt (line 3)) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.22.0->-r requirements.txt (line 3)) (1.26.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.22.0->-r requirements.txt (line 3)) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.22.0->-r requirements.txt (line 3)) (2.10)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers>=4.1.1->-r requirements.txt (line 4)) (0.10.3)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers>=4.1.1->-r requirements.txt (line 4)) (20.9)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers>=4.1.1->-r requirements.txt (line 4)) (3.7.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers>=4.1.1->-r requirements.txt (line 4)) (2020.11.13)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers>=4.1.1->-r requirements.txt (line 4)) (3.0.12)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers>=4.1.1->-r requirements.txt (line 4)) (0.8)\n",
      "Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers>=4.1.1->-r requirements.txt (line 4)) (0.0.45)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers>=4.1.1->-r requirements.txt (line 4)) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from huggingface-hub->sentence_transformers>=0.4.1.2->-r requirements.txt (line 1)) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from packaging->transformers>=4.1.1->-r requirements.txt (line 4)) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata->transformers>=4.1.1->-r requirements.txt (line 4)) (3.4.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from nltk->sentence_transformers>=0.4.1.2->-r requirements.txt (line 1)) (1.15.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sacremoses->transformers>=4.1.1->-r requirements.txt (line 4)) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sacremoses->transformers>=4.1.1->-r requirements.txt (line 4)) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from scikit-learn->sentence_transformers>=0.4.1.2->-r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: pillow>=5.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from torchvision->sentence_transformers>=0.4.1.2->-r requirements.txt (line 1)) (8.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.2.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4abe105b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: anytree in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.8.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from anytree) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.2.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install anytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e32814dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmt_retrieval.model.models import OSCAR, UNITER, M3P, ClassificationHead, Pooling\n",
    "from mmt_retrieval import MultimodalTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb17d33",
   "metadata": {},
   "source": [
    "# Loading pre-trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2357161",
   "metadata": {},
   "source": [
    "## M3P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f16f6d12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-08 13:50:02 - Lock 139649952230760 acquired on /home/ec2-user/.cache/huggingface/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516c4071d14d47daa3adc49950fd05d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/512 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-08 13:50:03 - Lock 139649952230760 released on /home/ec2-user/.cache/huggingface/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6.lock\n",
      "2021-08-08 13:50:03 - Lock 139649952230760 acquired on /home/ec2-user/.cache/huggingface/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "578167c759f34388a76872c3ea7f0666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-08 13:50:04 - Lock 139649952230760 released on /home/ec2-user/.cache/huggingface/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8.lock\n",
      "2021-08-08 13:50:04 - Lock 139649952230760 acquired on /home/ec2-user/.cache/huggingface/transformers/daeda8d936162ca65fe6dd158ecce1d8cb56c17d89b78ab86be1558eaef1d76a.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "657c73eea5f94bf781e471513447d67b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-08 13:50:05 - Lock 139649952230760 released on /home/ec2-user/.cache/huggingface/transformers/daeda8d936162ca65fe6dd158ecce1d8cb56c17d89b78ab86be1558eaef1d76a.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7.lock\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model M3P\n",
    "path_model_m3p = os.path.join(HOME_DIR, \"model_m3p/0_M3P\")\n",
    "pretrained_model = M3P(model_path = path_model_m3p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79d46e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[78011, 158, 91714, 35984, 1444]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model.tokenize(\"Pizza con champiñones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5042c1ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[78011, 158, 79, 52960, 24532, 1430]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model.tokenize(\"Pizza con jamón dulche\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94526c65",
   "metadata": {},
   "source": [
    "### Load image features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "641f9eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmt_retrieval.data.image_dict import ImageDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83c77542",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagedict = ImageDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cd7d3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.7 ms, sys: 3.57 ms, total: 38.3 ms\n",
      "Wall time: 37.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "imagedict.load_obj_tsv(\n",
    "    fname=\"/home/ec2-user/SageMaker/1_data/flickr30k_features/test_flickr30k_resnet101_faster_rcnn_genome.tsv.3\",\n",
    "    topk=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ccaa40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "value = imagedict[\"2842439618\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c295cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('img_id', '2842439618'),\n",
       "             ('img_w', 500),\n",
       "             ('img_h', 304),\n",
       "             ('num_boxes', 24),\n",
       "             ('boxes',\n",
       "              array([[209.26901  ,   9.549031 , 499.57333  , 295.8807   ],\n",
       "                     [121.21935  ,  38.891537 , 499.57333  , 249.98683  ],\n",
       "                     [  0.       ,  16.868515 , 263.36926  , 299.22125  ],\n",
       "                     [  0.       , 126.07229  , 463.2128   , 303.49335  ],\n",
       "                     [  0.       ,   0.       , 383.00916  , 225.9225   ],\n",
       "                     [169.05185  , 111.32224  , 231.4721   , 177.89626  ],\n",
       "                     [176.83896  , 141.99641  , 206.29744  , 167.80084  ],\n",
       "                     [183.13489  , 119.16288  , 220.69786  , 159.05319  ],\n",
       "                     [274.32715  ,  18.8361   , 499.57333  , 276.8327   ],\n",
       "                     [  0.       ,   0.       , 419.20593  ,  83.58264  ],\n",
       "                     [377.96402  , 220.5186   , 407.68393  , 242.58534  ],\n",
       "                     [ 62.589184 ,   6.4863286, 499.57333  ,  77.48054  ],\n",
       "                     [377.62366  , 216.44238  , 410.63553  , 238.38014  ],\n",
       "                     [ 67.04967  , 243.98233  , 499.57333  , 303.49335  ],\n",
       "                     [ 68.5005   ,  75.85149  , 439.75034  , 151.43996  ],\n",
       "                     [183.105    , 114.090454 , 203.87048  , 132.62033  ],\n",
       "                     [294.35052  ,  45.10415  , 499.57333  , 138.34622  ],\n",
       "                     [303.21854  , 284.26538  , 499.57333  , 303.49335  ],\n",
       "                     [159.76764  , 123.761635 , 248.2244   , 179.43974  ],\n",
       "                     [168.99202  , 155.99742  , 227.64444  , 228.82367  ],\n",
       "                     [171.12221  , 162.56563  , 214.64836  , 241.02008  ],\n",
       "                     [164.5239   , 137.06017  , 219.82614  , 188.95844  ],\n",
       "                     [354.47183  , 213.53217  , 433.07465  , 250.15686  ],\n",
       "                     [190.50081  , 101.18996  , 233.96309  , 159.82632  ]],\n",
       "                    dtype=float32)),\n",
       "             ('features',\n",
       "              array([[0.0000000e+00, 1.7515640e-03, 0.0000000e+00, ..., 2.8733835e-01,\n",
       "                      3.5628912e-01, 6.5357156e-02],\n",
       "                     [0.0000000e+00, 2.4161052e-02, 0.0000000e+00, ..., 6.1524314e-01,\n",
       "                      1.9609870e+00, 2.0857105e-01],\n",
       "                     [0.0000000e+00, 0.0000000e+00, 4.3634858e-02, ..., 3.1168818e-01,\n",
       "                      1.4189954e-01, 4.2050593e-03],\n",
       "                     ...,\n",
       "                     [6.3067985e-01, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
       "                      0.0000000e+00, 2.6016593e+00],\n",
       "                     [8.4934646e-01, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
       "                      3.4843526e+00, 5.1615381e-01],\n",
       "                     [1.7747411e+00, 0.0000000e+00, 0.0000000e+00, ..., 5.8235373e-02,\n",
       "                      2.0399225e+00, 6.8564093e-01]], dtype=float32))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagedict[\"2842439618\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "696d563b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 4)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coordinates of boxes\n",
    "imagedict[\"2842439618\"][\"boxes\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3302f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 2048)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# boxes features\n",
    "imagedict[\"2842439618\"][\"features\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a20a89c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(imagedict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e312fbd",
   "metadata": {},
   "source": [
    "# Run train test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd95d8b",
   "metadata": {},
   "source": [
    "See application notebook in MMT-Retrieval/examples/applications/Image_Search.ipynb \\\n",
    "**See code examples/experiments/run_train_test.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bac349f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/MMT-Retrieval\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cff357ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import yaml\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d05e213",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.load(open(\"examples/experiments/super_config.yaml\"), Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "662e5742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model paths\n",
    "path_model_m3p = os.path.join(HOME_DIR, \"model_m3p/0_M3P\")\n",
    "path_image_feature_folder = \"/home/ec2-user/SageMaker/1_data/flickr30k_features/tiny\"\n",
    "path_flickr30k_original_folder = \"/home/ec2-user/SageMaker/1_data/flickr30k_entities\"\n",
    "path_flickr_split_folder = \"/home/ec2-user/SageMaker/1_data/flickr30k_split_folder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cacd975f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######################################\n",
    "\n",
    "#   IMPORTANT!! modify config paths here\n",
    "\n",
    "# ######################################\n",
    "# Manual modifications\n",
    "config[\"model_path\"] = \"david-test\"\n",
    "\n",
    "# model name\n",
    "config[\"model\"][\"name\"] = \"m3p\"\n",
    "\n",
    "# Model path of the pretrained weights\n",
    "config[\"model\"][\"model_path\"] = path_model_m3p\n",
    "config[\"model\"][\"pretrained_model_path\"] = path_model_m3p\n",
    "\n",
    "# Path of the flickr features downloaded from  \n",
    "# https://drive.google.com/uc?export=download&id=11OD_qq7ITBarJwWZfi0bWIRw3HPEaHwE \n",
    "#(source: https://github.com/jnhwkim/ban-vqa/blob/master/tools/download_flickr.sh)\n",
    "config[\"data\"][\"image_feature_folder\"] = path_image_feature_folder\n",
    "config[\"data\"][\"flickr30k_original_folder\"] = path_flickr30k_original_folder\n",
    "config[\"data\"][\"flickr_split_folder\"] = path_flickr_split_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cf029c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "np.random.seed(config[\"seed\"])\n",
    "random.seed(config[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe16c4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model config\n",
    "model_config = config[\"model\"]\n",
    "\n",
    "# Creating folders\n",
    "model_folder_name = f\"{model_config['name']}-{datetime.now().strftime('%Y-%m-%d_%H-%M')}\"\n",
    "model_save_path = os.path.join(config[\"model_path\"], model_folder_name)\n",
    "os.makedirs(model_save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e92ca1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <TODO: CHANGE HERE> Put the path of our own model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0ac70a",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b9aaedd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Train'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indicate to Train\n",
    "config.get(\"do_train\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed3aa64",
   "metadata": {},
   "source": [
    "#### Output\"build_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e340d54e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_path': '/home/ec2-user/SageMaker/model_m3p/0_M3P',\n",
       " 'model_path_cross': 'YOUR_CROSS_ENCODER',\n",
       " 'model_path_embedding': 'YOUR_EMBEDDING_MODEL',\n",
       " 'name': 'm3p',\n",
       " 'pretrained_model_path': '/home/ec2-user/SageMaker/model_m3p/0_M3P',\n",
       " 'max_seq_length': 70,\n",
       " 'max_image_seq_len': 50,\n",
       " 'input_key': 'pooled_cls_token_embeddings',\n",
       " 'classifier_type': 'linear',\n",
       " 'scaling_factor': 1}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea73f0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-08 16:03:29 - Use pytorch device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "model = build_model(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7985e2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[72625, 9, 21854]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenize(\"Coca-cola\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba9ca102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[33112, 21049, 11, 158, 41, 991, 113, 33156, 1073]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenize(\"Hamburguesa con queso y patatas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d881d155",
   "metadata": {},
   "source": [
    "#### Deep dive \"build_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8810b04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_config):\n",
    "    if \"legacy\" not in model_config[\"name\"]:\n",
    "        if \"oscar\" in model_config[\"name\"]:\n",
    "            embedding_model = OSCAR(model_config[\"pretrained_model_path\"],\n",
    "                                    max_seq_length=model_config.get(\"max_seq_length\", 70),\n",
    "                                    max_image_seq_len=model_config.get(\"max_image_seq_len\", 50))\n",
    "        elif \"m3p\" in model_config[\"name\"]:\n",
    "            embedding_model = M3P(model_config[\"pretrained_model_path\"],\n",
    "                                  max_seq_length=model_config.get(\"max_seq_length\", 128),\n",
    "                                  max_image_seq_len=model_config.get(\"max_image_seq_len\", 50))\n",
    "        elif \"villa\" in model_config[\"name\"] or \"uniter\" in model_config[\"name\"]:\n",
    "            embedding_model = UNITER(model_config[\"pretrained_model_path\"],\n",
    "                                     max_seq_length=model_config.get(\"max_seq_length\", 70),\n",
    "                                     max_image_seq_len=model_config.get(\"max_image_seq_len\", 50))\n",
    "        if model_config.get(\"half_layers\", False):\n",
    "            module_list = torch.nn.ModuleList()\n",
    "            for i, layer in enumerate(embedding_model.auto_model.encoder.layer):\n",
    "                if i % 2 == 0:\n",
    "                    module_list.append(layer)\n",
    "            embedding_model.auto_model.encoder.layer = module_list\n",
    "\n",
    "        class_head = ClassificationHead(2, model_config.get(\"input_key\", \"pooled_cls_token_embeddings\"), 768,\n",
    "                                        model_config.get(\"classifier_type\", \"linear\"),\n",
    "                                        model_config.get(\"scaling_factor\", 2))\n",
    "        pooling_model = Pooling(768,\n",
    "                                       pooling_mode_mean_tokens=model_config.get(\"mean\", True),\n",
    "                                       pooling_mode_cls_token=model_config.get(\"cls\", False),\n",
    "                                       pooling_mode_max_tokens=model_config.get(\"max\", False))\n",
    "        return MultimodalTransformer(modules=[embedding_model, class_head, pooling_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f3edbf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <TODO: DIG DIVE INTO M3P initialization> (see model/models.py)\n",
    "embedding_model = M3P(\n",
    "    model_config[\"pretrained_model_path\"],\n",
    "    max_seq_length=model_config.get(\"max_seq_length\", 128),\n",
    "    max_image_seq_len=model_config.get(\"max_image_seq_len\", 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "faf53ce8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M3P(\n",
       "  (auto_model): TransformerModel(\n",
       "    (position_embeddings): Embedding(514, 768)\n",
       "    (embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "    (layer_norm_emb): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (image_embeddings): BertImageEmbeddings(\n",
       "      (image_embeddings): Linear(in_features=2048, out_features=768, bias=True)\n",
       "      (image_distbution_embeddings): Linear(in_features=1600, out_features=768, bias=True)\n",
       "      (image_location_embeddings): Linear(in_features=5, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (refine_embeddings): AoA_Refiner_Core(\n",
       "      (layers): ModuleList(\n",
       "        (0): AoA_Refiner_Layer(\n",
       "          (self_attn): MultiHeadedDotAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (aoa_layer): Sequential(\n",
       "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (1): GLU(dim=-1)\n",
       "            )\n",
       "            (dropout_aoa): Dropout(p=0.1, inplace=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (feed_forward): TransformerFFN(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (sublayer): ModuleList(\n",
       "            (0): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): AoA_Refiner_Layer(\n",
       "          (self_attn): MultiHeadedDotAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (aoa_layer): Sequential(\n",
       "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (1): GLU(dim=-1)\n",
       "            )\n",
       "            (dropout_aoa): Dropout(p=0.1, inplace=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (feed_forward): TransformerFFN(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (sublayer): ModuleList(\n",
       "            (0): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): AoA_Refiner_Layer(\n",
       "          (self_attn): MultiHeadedDotAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (aoa_layer): Sequential(\n",
       "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (1): GLU(dim=-1)\n",
       "            )\n",
       "            (dropout_aoa): Dropout(p=0.1, inplace=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (feed_forward): TransformerFFN(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (sublayer): ModuleList(\n",
       "            (0): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): AoA_Refiner_Layer(\n",
       "          (self_attn): MultiHeadedDotAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (aoa_layer): Sequential(\n",
       "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (1): GLU(dim=-1)\n",
       "            )\n",
       "            (dropout_aoa): Dropout(p=0.1, inplace=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (feed_forward): TransformerFFN(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (sublayer): ModuleList(\n",
       "            (0): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): AoA_Refiner_Layer(\n",
       "          (self_attn): MultiHeadedDotAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (aoa_layer): Sequential(\n",
       "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (1): GLU(dim=-1)\n",
       "            )\n",
       "            (dropout_aoa): Dropout(p=0.1, inplace=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (feed_forward): TransformerFFN(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (sublayer): ModuleList(\n",
       "            (0): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): AoA_Refiner_Layer(\n",
       "          (self_attn): MultiHeadedDotAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (aoa_layer): Sequential(\n",
       "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (1): GLU(dim=-1)\n",
       "            )\n",
       "            (dropout_aoa): Dropout(p=0.1, inplace=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (feed_forward): TransformerFFN(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (sublayer): ModuleList(\n",
       "            (0): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (cross_alignment): CrossAlignMatrix(\n",
       "      (att_weight_c): Linear(in_features=768, out_features=1, bias=True)\n",
       "      (att_weight_q): Linear(in_features=768, out_features=1, bias=True)\n",
       "      (att_weight_cq): Linear(in_features=768, out_features=1, bias=True)\n",
       "      (align_output): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (attentions): ModuleList(\n",
       "      (0): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (1): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (2): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (3): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (4): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (5): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (6): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (7): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (8): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (9): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (10): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (11): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm1): ModuleList(\n",
       "      (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (ffns): ModuleList(\n",
       "      (0): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (1): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (2): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (3): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (4): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (5): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (6): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (7): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (8): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (9): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (10): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (11): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm2): ModuleList(\n",
       "      (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (layer_norm15): ModuleList(\n",
       "      (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (encoder_attn): ModuleList(\n",
       "      (0): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (1): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (2): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (3): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (4): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (5): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (6): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (7): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (8): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (9): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (10): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (11): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (pooled_layer): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "    (seq_relationship): Linear(in_features=768, out_features=1, bias=True)\n",
       "    (mrfr_dense): Linear(in_features=768, out_features=2048, bias=True)\n",
       "    (transformer_obj): BertPredictionHeadTransform(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1f16adb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <TODO: DIG DIVE INTO ClassificationHead initialization> (see model/models.py)\n",
    "class_head = ClassificationHead(\n",
    "    2, model_config.get(\"input_key\", \"pooled_cls_token_embeddings\"), 768,\n",
    "    model_config.get(\"classifier_type\", \"linear\"),\n",
    "    model_config.get(\"scaling_factor\", 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a7eb7c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassificationHead(\n",
       "  (_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d268edc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pooled_cls_token_embeddings'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config.get(\"input_key\", \"pooled_cls_token_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8b9e398f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'linear'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config.get(\"classifier_type\", \"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "312a8932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config.get(\"scaling_factor\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6ac1ebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <TODO: DIG DIVE INTO ClassificationHead initialization> (see model/models.py)\n",
    "pooling_model = Pooling(\n",
    "    768,\n",
    "   pooling_mode_mean_tokens=model_config.get(\"mean\", True),\n",
    "   pooling_mode_cls_token=model_config.get(\"cls\", False),\n",
    "   pooling_mode_max_tokens=model_config.get(\"max\", False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e6b44a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config.get(\"cls\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "20773179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config.get(\"mean\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4f4a8a06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pooling()"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooling_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6050562a",
   "metadata": {},
   "source": [
    "## Task in train_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29f3f5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.experiments.process_data import get_sampler, DATA_LOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dec8f985",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = config[\"training\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77ab6d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = train_config[\"tasks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09f071d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'flickr30k',\n",
       "  'tiny': False,\n",
       "  'batchsize': 128,\n",
       "  'loss': {'name': 'triplet', 'margin': 0.1},\n",
       "  'data_args': {'jit_loading': False, 'languages': ['en', 'de', 'cs', 'fr']}},\n",
       " {'name': 'flickr30k',\n",
       "  'tiny': False,\n",
       "  'batchsize': 128,\n",
       "  'loss': {'name': 'joint'},\n",
       "  'data_args': {'jit_loading': False, 'languages': ['en', 'de', 'cs', 'fr']}},\n",
       " {'name': 'flickr30k',\n",
       "  'batchsize': 16,\n",
       "  'tiny': False,\n",
       "  'loss': {'name': 'ance', 'margin': 0.1},\n",
       "  'data_args': {'topk': 50, 'negative_examples': 7, 'sim_batchsize': 512}},\n",
       " {'name': 'flickr30k',\n",
       "  'tiny': False,\n",
       "  'batchsize': 128,\n",
       "  'do_hard': True,\n",
       "  'loss': {'name': 'joint'},\n",
       "  'data_args': {'sim_batchsize': 512, 'topk': 50, 'hard_p': 0.05}}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e34f006",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = tasks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ef203fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flickr30k'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task[\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0277a332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining get_data\n",
    "def get_data(data_config, name):\n",
    "    \"\"\"\n",
    "    for backwards compatibility of old configs\n",
    "    :param data_config:\n",
    "    :param name:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if \"all\" in data_config:\n",
    "        return data_config[\"all\"]\n",
    "    elif name in data_config:\n",
    "        return data_config[name]\n",
    "    else:\n",
    "        return data_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b09e5563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flickr30k'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task[\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8cca527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function examples.experiments.process_data.get_flickr30k(config, split, tiny, image_dict, **args)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_LOADER[task[\"name\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b9f1c5c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mscoco_folder': 'SEE_README',\n",
       " 'image_feature_folder': '/home/ec2-user/SageMaker/1_data/flickr30k_features/tiny',\n",
       " 'flickr_split_folder': '/home/ec2-user/SageMaker/1_data/flickr30k_split_folder',\n",
       " 'flickr30k_original_folder': '/home/ec2-user/SageMaker/1_data/flickr30k_entities',\n",
       " 'cc_original_folder': 'SEE_README',\n",
       " 'cc_feature_folder': 'SEE_README',\n",
       " 'multi30k_original_folder': 'SEE_README'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f8bea659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting get_flickr30k function (filling arguments of get_flickr30k) <ARGUMENTS>\n",
    "config_get = get_data(config[\"data\"], task[\"name\"])\n",
    "split = \"train\"\n",
    "tiny = task.get(\"tiny\", False)\n",
    "image_dict = model.image_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f52e0025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mscoco_folder': 'SEE_README',\n",
       " 'image_feature_folder': '/home/ec2-user/SageMaker/1_data/flickr30k_features/tiny',\n",
       " 'flickr_split_folder': '/home/ec2-user/SageMaker/1_data/flickr30k_split_folder',\n",
       " 'flickr30k_original_folder': '/home/ec2-user/SageMaker/1_data/flickr30k_entities',\n",
       " 'cc_original_folder': 'SEE_README',\n",
       " 'cc_feature_folder': 'SEE_README',\n",
       " 'multi30k_original_folder': 'SEE_README'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_data(config[\"data\"], task[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "87d705da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-08 16:04:56 - Flickr30k Image Feature Split does not exist. Creating in /home/ec2-user/SageMaker/1_data/flickr30k_split_folder\n",
      "reading  /home/ec2-user/SageMaker/1_data/flickr30k_features/tiny/test_flickr30k_resnet101_faster_rcnn_genome.tsv.3\n",
      "line 0\n",
      "reading  /home/ec2-user/SageMaker/1_data/flickr30k_features/tiny/train_flickr30k_resnet101_faster_rcnn_genome.tsv.1\n",
      "line 0\n",
      "line 500\n",
      "reading  /home/ec2-user/SageMaker/1_data/flickr30k_features/tiny/train_flickr30k_resnet101_faster_rcnn_genome.tsv.2\n",
      "line 0\n",
      "line 500\n",
      "reading  /home/ec2-user/SageMaker/1_data/flickr30k_features/tiny/val_flickr30k_resnet101_faster_rcnn_genome.tsv.3\n",
      "line 0\n",
      "2021-08-08 16:04:59 - Start to load Faster-RCNN detected objects from /home/ec2-user/SageMaker/1_data/flickr30k_split_folder/train_flickr30k_resnet101_faster_rcnn_genome.tsv\n",
      "CPU times: user 9.96 s, sys: 1.23 s, total: 11.2 s\n",
      "Wall time: 11.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset = DATA_LOADER[task[\"name\"]](\n",
    "    get_data(config[\"data\"], task[\"name\"]), \n",
    "    \"train\", \n",
    "    task.get(\"tiny\", False), \n",
    "    model.image_dict,\n",
    "    joint=task[\"loss\"][\"name\"] == \"joint\", \n",
    "    **task.get(\"data_args\", \n",
    "    {}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b56ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how this code created a folder named flickr30k_split_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "405183f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mmt_retrieval.data.datasets.ImageSentenceTripletDataset at 0x7f153b6b1e48>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "30822e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148915"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Caption keys\n",
    "len(dataset.caption_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7a46d621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148915"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Captions \n",
    "len(dataset.captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "64a49e21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['762678729_1', '762678729_2', '762678729_3', '762678729_4', '762678729_5']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.caption_keys[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "842541fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"a group of people , restrained by fencing , wait at the entrance to a tent ; there are signs at the entrance that read ' tent closed ' .\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.captions[\"762678729_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7b5b98d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "762678729_1\n",
      "762678729\n"
     ]
    }
   ],
   "source": [
    "item = 0\n",
    "pos_caption = dataset.caption_keys[item]\n",
    "print(pos_caption)\n",
    "pos_image = dataset.caption2image[pos_caption]\n",
    "print(pos_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9383cb47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([762678729])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label\n",
    "label = torch.LongTensor([int(pos_image)])\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "53d68e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"a group of people , restrained by fencing , wait at the entrance to a tent ; there are signs at the entrance that read ' tent closed ' .\",\n",
       " None)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Caption of image\n",
    "pos_caption = (dataset.captions[pos_caption], None)\n",
    "pos_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1ccf3ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# No tag\n",
    "tag = dataset.img2tag[pos_image] if dataset.tags else None\n",
    "print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "145e98f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, '762678729')\n"
     ]
    }
   ],
   "source": [
    "print((tag, pos_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6c606371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[(\"a group of people , restrained by fencing , wait at the entrance to a tent ; there are signs at the entrance that read ' tent closed ' .\",\n",
       "    None)]],\n",
       " [[(None, '762678729')]],\n",
       " tensor([762678729]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing __getitem__\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782ca0c9",
   "metadata": {},
   "source": [
    "### Get sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0274ca55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function examples.experiments.process_data.get_sampler(name, dataset, batchsize, **args)>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c71f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses the class MultidatasetImageSentenceTripletSampler in data > datasets.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3a9bafe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flickr30k'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task[\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "66df8da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.get(\"batchsize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "19ca6096",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler, batch_sampler, shuffle, batch_size = get_sampler(\n",
    "    task[\"name\"], \n",
    "    dataset,\n",
    "    task.get(\"batchsize\"), \n",
    "    **task.get(\"data_args\", {}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "25f6e641",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3304d470",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f678d1e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bdb36135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5adabd0",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1d1db674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4f9b1006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mmt_retrieval.data.datasets.ImageSentenceTripletDataset at 0x7f153b6b1e48>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d6802e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e5bc932f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a5dc3de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_config.get(\"num_workers\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9e0f7573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    shuffle=shuffle, \n",
    "    batch_size=batch_size, \n",
    "    sampler=sampler, \n",
    "    batch_sampler=batch_sampler, \n",
    "    num_workers=train_config.get(\"num_workers\", 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "706a23f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f159250f7b8>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69fbcbc",
   "metadata": {},
   "source": [
    "### Loss definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "77445f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmt_retrieval.losses.losses import BatchHardTripletLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "58fa13f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task[\"loss\"][\"name\"] == \"triplet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "96b4f1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task[\"loss\"].get(\"margin\", 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "034a771c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = BatchHardTripletLoss(model=model, margin=task[\"loss\"].get(\"margin\", 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "00a4437a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tasks = [(dataloader, loss)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd37aebd",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a3639876",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_config = train_config[\"dev\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32095680",
   "metadata": {},
   "source": [
    "#### Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e6adaeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmt_retrieval.evaluation.evaluations import EmbeddingImageTextRetrievalEvaluator, CrossEncoderImageTextRetrievalEvaluator, \\\n",
    "    RetrieveRerankImageTextRetrievalEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a909d079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluator(data_config, config, model, split):\n",
    "    split_evaluators = []\n",
    "    for task in config[\"tasks\"]:\n",
    "        load_split = task.get(\"overwrite_split\", split)\n",
    "        split_dataset = DATA_LOADER[task[\"name\"]](get_data(data_config, task[\"name\"]), load_split, task.get(\"tiny\", False),\n",
    "                                                  model.image_dict, return_dict=True, **task.get(\"data_args\", {}))\n",
    "        split_eval_name = task[\"evaluator\"][\"name\"]\n",
    "        file_name = f\"{split}-{task['name']}-{split_eval_name}\"\n",
    "        if split_eval_name == \"embedding_itr\" or split_eval_name == \"itr\":\n",
    "            evaluator = EmbeddingImageTextRetrievalEvaluator(split_dataset[\"images\"], split_dataset[\"captions\"], split_dataset[\"imageid2captions\"], split_dataset.get(\"tags\", None),\n",
    "                                                             name=file_name, show_progress_bar=True,\n",
    "                                                             batched_sim=task.get(\"batched_sim\", 0))\n",
    "        elif split_eval_name == \"ce_itr\" or split_eval_name == \"joint_itr\":\n",
    "            evaluator = CrossEncoderImageTextRetrievalEvaluator(split_dataset[\"images\"], split_dataset[\"captions\"], split_dataset[\"imageid2captions\"], split_dataset.get(\"tags\", None),\n",
    "                                                                name=file_name, batch_size=config[\"batchsize\"], show_progress_bar=True)\n",
    "        elif split_eval_name == \"rr_itr\":\n",
    "            evaluator = RetrieveRerankImageTextRetrievalEvaluator(split_dataset[\"images\"], split_dataset[\"captions\"], split_dataset[\"imageid2captions\"], split_dataset.get(\"tags\", None),\n",
    "                                                                  name=file_name, batch_size=config[\"batchsize\"], show_progress_bar=True,\n",
    "                                                                  retrieve=task.get(\"retrieve\", 10), scoring=task.get(\"scoring\", \"standard\"),\n",
    "                                                                  scoring_factor=task.get(\"scoring_factor\", 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0c2ff548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tasks': [{'name': 'flickr30k',\n",
       "   'tiny': False,\n",
       "   'evaluator': {'name': 'embedding_itr'},\n",
       "   'data_args': {'languages': ['en']}},\n",
       "  {'name': 'flickr30k',\n",
       "   'tiny': True,\n",
       "   'evaluator': {'name': 'ce_itr'},\n",
       "   'data_args': {'captions_per_image': 1, 'languages': ['en']}}],\n",
       " 'batchsize': 512,\n",
       " 'main_score_function': 'mean'}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6cb57a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mscoco_folder': 'SEE_README',\n",
       " 'image_feature_folder': '/home/ec2-user/SageMaker/1_data/flickr30k_features/tiny',\n",
       " 'flickr_split_folder': '/home/ec2-user/SageMaker/1_data/flickr30k_split_folder',\n",
       " 'flickr30k_original_folder': '/home/ec2-user/SageMaker/1_data/flickr30k_entities',\n",
       " 'cc_original_folder': 'SEE_README',\n",
       " 'cc_feature_folder': 'SEE_README',\n",
       " 'multi30k_original_folder': 'SEE_README'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f2056cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-08 16:32:27 - Flickr30k Image Feature Split exists. Loading from /home/ec2-user/SageMaker/1_data/flickr30k_split_folder\n",
      "2021-08-08 16:32:27 - Start to load Faster-RCNN detected objects from /home/ec2-user/SageMaker/1_data/flickr30k_split_folder/dev_flickr30k_resnet101_faster_rcnn_genome.tsv\n",
      "2021-08-08 16:32:28 - Flickr30k Image Feature Split exists. Loading from /home/ec2-user/SageMaker/1_data/flickr30k_split_folder\n",
      "2021-08-08 16:32:28 - Start to load Faster-RCNN detected objects from /home/ec2-user/SageMaker/1_data/flickr30k_split_folder/dev_flickr30k_resnet101_faster_rcnn_genome.tsv\n"
     ]
    }
   ],
   "source": [
    "dev_evaluator = get_evaluator(config[\"data\"], dev_config, model, \"dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44e6613",
   "metadata": {},
   "source": [
    "##### Deep dive into get_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd33ca54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "optimizer_class = transformers.AdamW\n",
    "optimizer_params={\"lr\": train_config.get(\"lr\", 2e-5), \"eps\": train_config.get(\"eps\", 1e-6)}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
