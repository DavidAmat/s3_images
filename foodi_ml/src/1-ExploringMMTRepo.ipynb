{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4879d9f4",
   "metadata": {},
   "source": [
    "# Exploration on MMT Retrieval repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c32f4599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HOME_DIR = \"/home/ec2-user/SageMaker\"\n",
    "os.chdir(f\"{HOME_DIR}/MMT-Retrieval/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaf8addb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence_transformers>=0.4.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (2.0.0)\n",
      "Requirement already satisfied: tqdm>=4.32.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (4.62.0)\n",
      "Requirement already satisfied: requests>=2.22.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (2.25.1)\n",
      "Requirement already satisfied: transformers>=4.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (4.9.1)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from -r requirements.txt (line 5)) (1.19.5)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from -r requirements.txt (line 6)) (1.9.0)\n",
      "Requirement already satisfied: huggingface-hub in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sentence_transformers>=0.4.1.2->-r requirements.txt (line 1)) (0.0.12)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sentence_transformers>=0.4.1.2->-r requirements.txt (line 1)) (0.10.0)\n",
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sentence_transformers>=0.4.1.2->-r requirements.txt (line 1)) (3.4.4)\n",
      "Requirement already satisfied: sentencepiece in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sentence_transformers>=0.4.1.2->-r requirements.txt (line 1)) (0.1.96)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sentence_transformers>=0.4.1.2->-r requirements.txt (line 1)) (1.5.3)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sentence_transformers>=0.4.1.2->-r requirements.txt (line 1)) (0.24.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.22.0->-r requirements.txt (line 3)) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.22.0->-r requirements.txt (line 3)) (1.26.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.22.0->-r requirements.txt (line 3)) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.22.0->-r requirements.txt (line 3)) (2.10)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers>=4.1.1->-r requirements.txt (line 4)) (0.10.3)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers>=4.1.1->-r requirements.txt (line 4)) (20.9)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers>=4.1.1->-r requirements.txt (line 4)) (3.7.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers>=4.1.1->-r requirements.txt (line 4)) (2020.11.13)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers>=4.1.1->-r requirements.txt (line 4)) (3.0.12)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers>=4.1.1->-r requirements.txt (line 4)) (0.8)\n",
      "Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers>=4.1.1->-r requirements.txt (line 4)) (0.0.45)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers>=4.1.1->-r requirements.txt (line 4)) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from huggingface-hub->sentence_transformers>=0.4.1.2->-r requirements.txt (line 1)) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from packaging->transformers>=4.1.1->-r requirements.txt (line 4)) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata->transformers>=4.1.1->-r requirements.txt (line 4)) (3.4.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from nltk->sentence_transformers>=0.4.1.2->-r requirements.txt (line 1)) (1.15.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sacremoses->transformers>=4.1.1->-r requirements.txt (line 4)) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sacremoses->transformers>=4.1.1->-r requirements.txt (line 4)) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from scikit-learn->sentence_transformers>=0.4.1.2->-r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: pillow>=5.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from torchvision->sentence_transformers>=0.4.1.2->-r requirements.txt (line 1)) (8.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.2.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "085340fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: anytree in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.8.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from anytree) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.2.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install anytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cf35585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmt_retrieval.model.models import OSCAR, UNITER, M3P, ClassificationHead, Pooling\n",
    "from mmt_retrieval import MultimodalTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0681cad",
   "metadata": {},
   "source": [
    "# Loading pre-trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1559a36",
   "metadata": {},
   "source": [
    "## M3P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0105f85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-08 13:50:02 - Lock 139649952230760 acquired on /home/ec2-user/.cache/huggingface/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516c4071d14d47daa3adc49950fd05d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/512 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-08 13:50:03 - Lock 139649952230760 released on /home/ec2-user/.cache/huggingface/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6.lock\n",
      "2021-08-08 13:50:03 - Lock 139649952230760 acquired on /home/ec2-user/.cache/huggingface/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "578167c759f34388a76872c3ea7f0666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-08 13:50:04 - Lock 139649952230760 released on /home/ec2-user/.cache/huggingface/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8.lock\n",
      "2021-08-08 13:50:04 - Lock 139649952230760 acquired on /home/ec2-user/.cache/huggingface/transformers/daeda8d936162ca65fe6dd158ecce1d8cb56c17d89b78ab86be1558eaef1d76a.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "657c73eea5f94bf781e471513447d67b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-08 13:50:05 - Lock 139649952230760 released on /home/ec2-user/.cache/huggingface/transformers/daeda8d936162ca65fe6dd158ecce1d8cb56c17d89b78ab86be1558eaef1d76a.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7.lock\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model M3P\n",
    "path_model_m3p = os.path.join(HOME_DIR, \"model_m3p/0_M3P\")\n",
    "pretrained_model = M3P(model_path = path_model_m3p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0e6b4cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[78011, 158, 91714, 35984, 1444]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model.tokenize(\"Pizza con champiñones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "666b439f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[78011, 158, 79, 52960, 24532, 1430]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model.tokenize(\"Pizza con jamón dulche\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea45e3f",
   "metadata": {},
   "source": [
    "### Load image features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "f0e82976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmt_retrieval.data.image_embeddings import ImageDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "23232f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagedict = ImageDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "e1b96c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.2 ms, sys: 67 µs, total: 38.2 ms\n",
      "Wall time: 35 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "imagedict.load_obj_tsv(\n",
    "    fname=\"/home/ec2-user/SageMaker/1_data/flickr30k_features/tiny/train_flickr30k_resnet101_faster_rcnn_genome.tsv.1\",\n",
    "    topk=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "27adfc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "value = imagedict[\"1213185795\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "41b993f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('img_id', '1213185795'),\n",
       "             ('img_w', 500),\n",
       "             ('img_h', 333),\n",
       "             ('num_boxes', 39),\n",
       "             ('boxes',\n",
       "              array([[178.77786 ,   0.      , 499.5     , 189.06422 ],\n",
       "                     [286.2556  ,  91.49405 , 409.02844 , 224.29764 ],\n",
       "                     [  0.      ,  17.339499, 211.28693 , 317.61197 ],\n",
       "                     [  6.189648,  34.51717 , 108.51078 , 292.2112  ],\n",
       "                     [319.35147 , 286.56503 , 466.34363 , 332.445   ],\n",
       "                     [323.2654  ,  30.65785 , 477.2827  ,  78.58263 ],\n",
       "                     [263.98987 , 235.54553 , 499.5     , 332.445   ],\n",
       "                     [  0.      ,   0.      , 316.1118  ,  54.290844],\n",
       "                     [203.84828 , 190.30222 , 499.5     , 277.82635 ],\n",
       "                     [229.45714 , 221.64037 , 275.42932 , 300.42758 ],\n",
       "                     [119.19319 ,   0.      , 427.99127 , 332.445   ],\n",
       "                     [ 42.994564,   9.276467, 231.20543 , 332.445   ],\n",
       "                     [100.72046 , 167.77585 , 187.42458 , 332.445   ],\n",
       "                     [455.25516 ,   9.078192, 496.60724 , 101.476776],\n",
       "                     [  0.      , 299.91238 , 100.33168 , 332.445   ],\n",
       "                     [  0.      ,  30.10659 ,  93.24412 , 242.43422 ],\n",
       "                     [298.9117  , 131.05226 , 399.33704 , 199.45631 ],\n",
       "                     [301.5103  ,  40.82225 , 469.55792 , 196.3782  ],\n",
       "                     [163.97351 , 205.35411 , 214.86119 , 322.90744 ],\n",
       "                     [109.22744 , 130.98517 , 195.39713 , 187.38329 ],\n",
       "                     [331.18268 , 103.95835 , 364.7279  , 125.00733 ],\n",
       "                     [172.76146 ,   0.      , 448.96564 ,  70.21708 ],\n",
       "                     [398.46008 , 186.65256 , 441.28268 , 234.32088 ],\n",
       "                     [269.0137  ,  71.28347 , 296.1825  , 116.834984],\n",
       "                     [106.548584,  54.401344, 146.73668 ,  92.224106],\n",
       "                     [107.1162  , 121.05843 , 181.70676 , 194.04994 ],\n",
       "                     [184.49614 ,  37.30108 , 212.90971 ,  80.327484],\n",
       "                     [448.3342  , 197.78416 , 484.78122 , 234.24266 ],\n",
       "                     [ 87.02091 ,  15.206438, 336.498   , 135.91487 ],\n",
       "                     [375.79132 ,  66.140854, 394.2036  ,  86.708595],\n",
       "                     [108.09845 ,  98.417786, 152.9292  , 137.53912 ],\n",
       "                     [130.47333 , 139.71098 , 181.07495 , 176.97809 ],\n",
       "                     [392.9032  ,   0.      , 499.5     , 258.28555 ],\n",
       "                     [329.4981  , 109.35234 , 362.16846 , 138.67793 ],\n",
       "                     [158.03471 , 105.09128 , 189.18188 , 132.92972 ],\n",
       "                     [464.4164  ,   6.426557, 495.07474 ,  23.493181],\n",
       "                     [431.10962 , 187.02502 , 497.26797 , 246.83586 ],\n",
       "                     [115.174675, 182.58179 , 150.28006 , 332.445   ],\n",
       "                     [430.56476 , 227.17807 , 450.7564  , 245.1039  ]], dtype=float32)),\n",
       "             ('features',\n",
       "              array([[4.4259453e-01, 4.5629039e-01, 2.2752611e-01, ..., 6.9394588e-01,\n",
       "                      4.2044764e+00, 0.0000000e+00],\n",
       "                     [1.0395095e+00, 0.0000000e+00, 0.0000000e+00, ..., 4.5075051e-02,\n",
       "                      4.0495762e-01, 0.0000000e+00],\n",
       "                     [2.7406731e+00, 2.4304210e-01, 0.0000000e+00, ..., 4.4396430e-01,\n",
       "                      0.0000000e+00, 2.7462265e-01],\n",
       "                     ...,\n",
       "                     [0.0000000e+00, 5.2186966e+00, 0.0000000e+00, ..., 4.1836292e-02,\n",
       "                      2.7950516e-03, 8.6152625e-01],\n",
       "                     [1.5662067e+00, 5.3293669e-01, 1.5880404e-02, ..., 0.0000000e+00,\n",
       "                      0.0000000e+00, 3.2213101e-01],\n",
       "                     [4.0294129e-02, 7.2604384e+00, 9.3506306e-02, ..., 0.0000000e+00,\n",
       "                      0.0000000e+00, 8.3531260e-01]], dtype=float32))])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagedict[\"1213185795\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "c2c7d4e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 4)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coordinates of boxes\n",
    "imagedict[\"1213185795\"][\"boxes\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "3c3ad789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 2048)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# boxes features\n",
    "imagedict[\"1213185795\"][\"features\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c5cf0ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(imagedict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea4e1d3",
   "metadata": {},
   "source": [
    "# Run train test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894747a7",
   "metadata": {},
   "source": [
    "See application notebook in MMT-Retrieval/examples/applications/Image_Search.ipynb \\\n",
    "**See code examples/experiments/run_train_test.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12b11a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/MMT-Retrieval\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1015842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import yaml\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ae69aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.load(open(\"examples/experiments/super_config.yaml\"), Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb017723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model paths\n",
    "path_model_m3p = os.path.join(HOME_DIR, \"model_m3p/0_M3P\")\n",
    "path_image_feature_folder = \"/home/ec2-user/SageMaker/1_data/flickr30k_features/tiny\"\n",
    "path_flickr30k_original_folder = \"/home/ec2-user/SageMaker/1_data/flickr30k_entities\"\n",
    "path_flickr_split_folder = \"/home/ec2-user/SageMaker/1_data/flickr30k_split_folder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1436a74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######################################\n",
    "\n",
    "#   IMPORTANT!! modify config paths here\n",
    "\n",
    "# ######################################\n",
    "# Manual modifications\n",
    "config[\"model_path\"] = \"david-test\"\n",
    "\n",
    "# model name\n",
    "config[\"model\"][\"name\"] = \"m3p\"\n",
    "\n",
    "# Model path of the pretrained weights\n",
    "config[\"model\"][\"model_path\"] = path_model_m3p\n",
    "config[\"model\"][\"pretrained_model_path\"] = path_model_m3p\n",
    "\n",
    "# Path of the flickr features downloaded from  \n",
    "# https://drive.google.com/uc?export=download&id=11OD_qq7ITBarJwWZfi0bWIRw3HPEaHwE \n",
    "#(source: https://github.com/jnhwkim/ban-vqa/blob/master/tools/download_flickr.sh)\n",
    "config[\"data\"][\"image_feature_folder\"] = path_image_feature_folder\n",
    "config[\"data\"][\"flickr30k_original_folder\"] = path_flickr30k_original_folder\n",
    "config[\"data\"][\"flickr_split_folder\"] = path_flickr_split_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f101847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "np.random.seed(config[\"seed\"])\n",
    "random.seed(config[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5e3e6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model config\n",
    "model_config = config[\"model\"]\n",
    "\n",
    "# Creating folders\n",
    "model_folder_name = f\"{model_config['name']}-{datetime.now().strftime('%Y-%m-%d_%H-%M')}\"\n",
    "model_save_path = os.path.join(config[\"model_path\"], model_folder_name)\n",
    "os.makedirs(model_save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8d8c5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <TODO: CHANGE HERE> Put the path of our own model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe32656",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48306fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Train'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indicate to Train\n",
    "config.get(\"do_train\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c2f114",
   "metadata": {},
   "source": [
    "#### Output\"build_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0154b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_path': '/home/ec2-user/SageMaker/model_m3p/0_M3P',\n",
       " 'model_path_cross': 'YOUR_CROSS_ENCODER',\n",
       " 'model_path_embedding': 'YOUR_EMBEDDING_MODEL',\n",
       " 'name': 'm3p',\n",
       " 'pretrained_model_path': '/home/ec2-user/SageMaker/model_m3p/0_M3P',\n",
       " 'max_seq_length': 70,\n",
       " 'max_image_seq_len': 50,\n",
       " 'input_key': 'pooled_cls_token_embeddings',\n",
       " 'classifier_type': 'linear',\n",
       " 'scaling_factor': 1}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e924a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-08 16:03:29 - Use pytorch device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "model = build_model(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "281dd527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[72625, 9, 21854]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenize(\"Coca-cola\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3877771e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[33112, 21049, 11, 158, 41, 991, 113, 33156, 1073]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenize(\"Hamburguesa con queso y patatas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76457096",
   "metadata": {},
   "source": [
    "#### Deep dive \"build_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c54262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_config):\n",
    "    if \"legacy\" not in model_config[\"name\"]:\n",
    "        if \"oscar\" in model_config[\"name\"]:\n",
    "            embedding_model = OSCAR(model_config[\"pretrained_model_path\"],\n",
    "                                    max_seq_length=model_config.get(\"max_seq_length\", 70),\n",
    "                                    max_image_seq_len=model_config.get(\"max_image_seq_len\", 50))\n",
    "        elif \"m3p\" in model_config[\"name\"]:\n",
    "            embedding_model = M3P(model_config[\"pretrained_model_path\"],\n",
    "                                  max_seq_length=model_config.get(\"max_seq_length\", 128),\n",
    "                                  max_image_seq_len=model_config.get(\"max_image_seq_len\", 50))\n",
    "        elif \"villa\" in model_config[\"name\"] or \"uniter\" in model_config[\"name\"]:\n",
    "            embedding_model = UNITER(model_config[\"pretrained_model_path\"],\n",
    "                                     max_seq_length=model_config.get(\"max_seq_length\", 70),\n",
    "                                     max_image_seq_len=model_config.get(\"max_image_seq_len\", 50))\n",
    "        if model_config.get(\"half_layers\", False):\n",
    "            module_list = torch.nn.ModuleList()\n",
    "            for i, layer in enumerate(embedding_model.auto_model.encoder.layer):\n",
    "                if i % 2 == 0:\n",
    "                    module_list.append(layer)\n",
    "            embedding_model.auto_model.encoder.layer = module_list\n",
    "\n",
    "        class_head = ClassificationHead(2, model_config.get(\"input_key\", \"pooled_cls_token_embeddings\"), 768,\n",
    "                                        model_config.get(\"classifier_type\", \"linear\"),\n",
    "                                        model_config.get(\"scaling_factor\", 2))\n",
    "        pooling_model = Pooling(768,\n",
    "                                       pooling_mode_mean_tokens=model_config.get(\"mean\", True),\n",
    "                                       pooling_mode_cls_token=model_config.get(\"cls\", False),\n",
    "                                       pooling_mode_max_tokens=model_config.get(\"max\", False))\n",
    "        return MultimodalTransformer(modules=[embedding_model, class_head, pooling_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "222dbe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <TODO: DIG DIVE INTO M3P initialization> (see model/models.py)\n",
    "embedding_model = M3P(\n",
    "    model_config[\"pretrained_model_path\"],\n",
    "    max_seq_length=model_config.get(\"max_seq_length\", 128),\n",
    "    max_image_seq_len=model_config.get(\"max_image_seq_len\", 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7cf07822",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M3P(\n",
       "  (auto_model): TransformerModel(\n",
       "    (position_embeddings): Embedding(514, 768)\n",
       "    (embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "    (layer_norm_emb): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (image_embeddings): BertImageEmbeddings(\n",
       "      (image_embeddings): Linear(in_features=2048, out_features=768, bias=True)\n",
       "      (image_distbution_embeddings): Linear(in_features=1600, out_features=768, bias=True)\n",
       "      (image_location_embeddings): Linear(in_features=5, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (refine_embeddings): AoA_Refiner_Core(\n",
       "      (layers): ModuleList(\n",
       "        (0): AoA_Refiner_Layer(\n",
       "          (self_attn): MultiHeadedDotAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (aoa_layer): Sequential(\n",
       "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (1): GLU(dim=-1)\n",
       "            )\n",
       "            (dropout_aoa): Dropout(p=0.1, inplace=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (feed_forward): TransformerFFN(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (sublayer): ModuleList(\n",
       "            (0): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): AoA_Refiner_Layer(\n",
       "          (self_attn): MultiHeadedDotAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (aoa_layer): Sequential(\n",
       "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (1): GLU(dim=-1)\n",
       "            )\n",
       "            (dropout_aoa): Dropout(p=0.1, inplace=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (feed_forward): TransformerFFN(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (sublayer): ModuleList(\n",
       "            (0): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): AoA_Refiner_Layer(\n",
       "          (self_attn): MultiHeadedDotAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (aoa_layer): Sequential(\n",
       "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (1): GLU(dim=-1)\n",
       "            )\n",
       "            (dropout_aoa): Dropout(p=0.1, inplace=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (feed_forward): TransformerFFN(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (sublayer): ModuleList(\n",
       "            (0): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): AoA_Refiner_Layer(\n",
       "          (self_attn): MultiHeadedDotAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (aoa_layer): Sequential(\n",
       "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (1): GLU(dim=-1)\n",
       "            )\n",
       "            (dropout_aoa): Dropout(p=0.1, inplace=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (feed_forward): TransformerFFN(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (sublayer): ModuleList(\n",
       "            (0): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): AoA_Refiner_Layer(\n",
       "          (self_attn): MultiHeadedDotAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (aoa_layer): Sequential(\n",
       "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (1): GLU(dim=-1)\n",
       "            )\n",
       "            (dropout_aoa): Dropout(p=0.1, inplace=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (feed_forward): TransformerFFN(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (sublayer): ModuleList(\n",
       "            (0): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): AoA_Refiner_Layer(\n",
       "          (self_attn): MultiHeadedDotAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (aoa_layer): Sequential(\n",
       "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (1): GLU(dim=-1)\n",
       "            )\n",
       "            (dropout_aoa): Dropout(p=0.1, inplace=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (feed_forward): TransformerFFN(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (sublayer): ModuleList(\n",
       "            (0): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (cross_alignment): CrossAlignMatrix(\n",
       "      (att_weight_c): Linear(in_features=768, out_features=1, bias=True)\n",
       "      (att_weight_q): Linear(in_features=768, out_features=1, bias=True)\n",
       "      (att_weight_cq): Linear(in_features=768, out_features=1, bias=True)\n",
       "      (align_output): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (attentions): ModuleList(\n",
       "      (0): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (1): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (2): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (3): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (4): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (5): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (6): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (7): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (8): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (9): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (10): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (11): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm1): ModuleList(\n",
       "      (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (ffns): ModuleList(\n",
       "      (0): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (1): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (2): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (3): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (4): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (5): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (6): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (7): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (8): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (9): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (10): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (11): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm2): ModuleList(\n",
       "      (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (layer_norm15): ModuleList(\n",
       "      (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (encoder_attn): ModuleList(\n",
       "      (0): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (1): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (2): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (3): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (4): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (5): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (6): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (7): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (8): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (9): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (10): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (11): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (pooled_layer): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "    (seq_relationship): Linear(in_features=768, out_features=1, bias=True)\n",
       "    (mrfr_dense): Linear(in_features=768, out_features=2048, bias=True)\n",
       "    (transformer_obj): BertPredictionHeadTransform(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "48cf9d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <TODO: DIG DIVE INTO ClassificationHead initialization> (see model/models.py)\n",
    "class_head = ClassificationHead(\n",
    "    2, model_config.get(\"input_key\", \"pooled_cls_token_embeddings\"), 768,\n",
    "    model_config.get(\"classifier_type\", \"linear\"),\n",
    "    model_config.get(\"scaling_factor\", 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a17575cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassificationHead(\n",
       "  (_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c6518f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pooled_cls_token_embeddings'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config.get(\"input_key\", \"pooled_cls_token_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "968b3814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'linear'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config.get(\"classifier_type\", \"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "73e715b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config.get(\"scaling_factor\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c1456e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <TODO: DIG DIVE INTO ClassificationHead initialization> (see model/models.py)\n",
    "pooling_model = Pooling(\n",
    "    768,\n",
    "   pooling_mode_mean_tokens=model_config.get(\"mean\", True),\n",
    "   pooling_mode_cls_token=model_config.get(\"cls\", False),\n",
    "   pooling_mode_max_tokens=model_config.get(\"max\", False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "44203ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config.get(\"cls\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0f751a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config.get(\"mean\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "da25d795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pooling()"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooling_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bea6f0a",
   "metadata": {},
   "source": [
    "## Task in train_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b07a4ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.experiments.process_data import get_sampler, DATA_LOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "548b32fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = config[\"training\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "412cc838",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = train_config[\"tasks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17f55c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'flickr30k',\n",
       "  'tiny': False,\n",
       "  'batchsize': 128,\n",
       "  'loss': {'name': 'triplet', 'margin': 0.1},\n",
       "  'data_args': {'jit_loading': False, 'languages': ['en', 'de', 'cs', 'fr']}},\n",
       " {'name': 'flickr30k',\n",
       "  'tiny': False,\n",
       "  'batchsize': 128,\n",
       "  'loss': {'name': 'joint'},\n",
       "  'data_args': {'jit_loading': False, 'languages': ['en', 'de', 'cs', 'fr']}},\n",
       " {'name': 'flickr30k',\n",
       "  'batchsize': 16,\n",
       "  'tiny': False,\n",
       "  'loss': {'name': 'ance', 'margin': 0.1},\n",
       "  'data_args': {'topk': 50, 'negative_examples': 7, 'sim_batchsize': 512}},\n",
       " {'name': 'flickr30k',\n",
       "  'tiny': False,\n",
       "  'batchsize': 128,\n",
       "  'do_hard': True,\n",
       "  'loss': {'name': 'joint'},\n",
       "  'data_args': {'sim_batchsize': 512, 'topk': 50, 'hard_p': 0.05}}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74ebb6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = tasks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c4a73b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flickr30k'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task[\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "918443c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining get_data\n",
    "def get_data(data_config, name):\n",
    "    \"\"\"\n",
    "    for backwards compatibility of old configs\n",
    "    :param data_config:\n",
    "    :param name:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if \"all\" in data_config:\n",
    "        return data_config[\"all\"]\n",
    "    elif name in data_config:\n",
    "        return data_config[name]\n",
    "    else:\n",
    "        return data_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3bd07c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flickr30k'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task[\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7eca55e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function examples.experiments.process_data.get_flickr30k(config, split, tiny, image_dict, **args)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_LOADER[task[\"name\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "caaef9f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mscoco_folder': 'SEE_README',\n",
       " 'image_feature_folder': '/home/ec2-user/SageMaker/1_data/flickr30k_features/tiny',\n",
       " 'flickr_split_folder': '/home/ec2-user/SageMaker/1_data/flickr30k_split_folder',\n",
       " 'flickr30k_original_folder': '/home/ec2-user/SageMaker/1_data/flickr30k_entities',\n",
       " 'cc_original_folder': 'SEE_README',\n",
       " 'cc_feature_folder': 'SEE_README',\n",
       " 'multi30k_original_folder': 'SEE_README'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "724c30ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting get_flickr30k function (filling arguments of get_flickr30k) <ARGUMENTS>\n",
    "config_get = get_data(config[\"data\"], task[\"name\"])\n",
    "split = \"train\"\n",
    "tiny = task.get(\"tiny\", False)\n",
    "image_dict = model.image_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "47c95741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mscoco_folder': 'SEE_README',\n",
       " 'image_feature_folder': '/home/ec2-user/SageMaker/1_data/flickr30k_features/tiny',\n",
       " 'flickr_split_folder': '/home/ec2-user/SageMaker/1_data/flickr30k_split_folder',\n",
       " 'flickr30k_original_folder': '/home/ec2-user/SageMaker/1_data/flickr30k_entities',\n",
       " 'cc_original_folder': 'SEE_README',\n",
       " 'cc_feature_folder': 'SEE_README',\n",
       " 'multi30k_original_folder': 'SEE_README'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_data(config[\"data\"], task[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c362a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-08 16:04:56 - Flickr30k Image Feature Split does not exist. Creating in /home/ec2-user/SageMaker/1_data/flickr30k_split_folder\n",
      "reading  /home/ec2-user/SageMaker/1_data/flickr30k_features/tiny/test_flickr30k_resnet101_faster_rcnn_genome.tsv.3\n",
      "line 0\n",
      "reading  /home/ec2-user/SageMaker/1_data/flickr30k_features/tiny/train_flickr30k_resnet101_faster_rcnn_genome.tsv.1\n",
      "line 0\n",
      "line 500\n",
      "reading  /home/ec2-user/SageMaker/1_data/flickr30k_features/tiny/train_flickr30k_resnet101_faster_rcnn_genome.tsv.2\n",
      "line 0\n",
      "line 500\n",
      "reading  /home/ec2-user/SageMaker/1_data/flickr30k_features/tiny/val_flickr30k_resnet101_faster_rcnn_genome.tsv.3\n",
      "line 0\n",
      "2021-08-08 16:04:59 - Start to load Faster-RCNN detected objects from /home/ec2-user/SageMaker/1_data/flickr30k_split_folder/train_flickr30k_resnet101_faster_rcnn_genome.tsv\n",
      "CPU times: user 9.96 s, sys: 1.23 s, total: 11.2 s\n",
      "Wall time: 11.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset = DATA_LOADER[task[\"name\"]](\n",
    "    get_data(config[\"data\"], task[\"name\"]), \n",
    "    \"train\", \n",
    "    task.get(\"tiny\", False), \n",
    "    model.image_dict,\n",
    "    joint=task[\"loss\"][\"name\"] == \"joint\", \n",
    "    **task.get(\"data_args\", \n",
    "    {}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a7c63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how this code created a folder named flickr30k_split_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "19124609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mmt_retrieval.data.datasets.ImageSentenceTripletDataset at 0x7f153b6b1e48>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2c998772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148915"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Caption keys\n",
    "len(dataset.caption_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "afa91b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148915"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Captions \n",
    "len(dataset.captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a2276edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['762678729_1', '762678729_2', '762678729_3', '762678729_4', '762678729_5']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.caption_keys[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "86328625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"a group of people , restrained by fencing , wait at the entrance to a tent ; there are signs at the entrance that read ' tent closed ' .\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.captions[\"762678729_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "78b0a64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "762678729_1\n",
      "762678729\n"
     ]
    }
   ],
   "source": [
    "item = 0\n",
    "pos_caption = dataset.caption_keys[item]\n",
    "print(pos_caption)\n",
    "pos_image = dataset.caption2image[pos_caption]\n",
    "print(pos_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0b686c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([762678729])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label\n",
    "label = torch.LongTensor([int(pos_image)])\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c05e54df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"a group of people , restrained by fencing , wait at the entrance to a tent ; there are signs at the entrance that read ' tent closed ' .\",\n",
       " None)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Caption of image\n",
    "pos_caption = (dataset.captions[pos_caption], None)\n",
    "pos_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d000f61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# No tag\n",
    "tag = dataset.img2tag[pos_image] if dataset.tags else None\n",
    "print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4ec2ea59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, '762678729')\n"
     ]
    }
   ],
   "source": [
    "print((tag, pos_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e14a8525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[(\"a group of people , restrained by fencing , wait at the entrance to a tent ; there are signs at the entrance that read ' tent closed ' .\",\n",
       "    None)]],\n",
       " [[(None, '762678729')]],\n",
       " tensor([762678729]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing __getitem__\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a12b639",
   "metadata": {},
   "source": [
    "### Get sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3501cebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function examples.experiments.process_data.get_sampler(name, dataset, batchsize, **args)>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9adff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses the class MultidatasetImageSentenceTripletSampler in data > datasets.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ae389c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flickr30k'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task[\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bf7e766a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.get(\"batchsize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "95ebffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler, batch_sampler, shuffle, batch_size = get_sampler(\n",
    "    task[\"name\"], \n",
    "    dataset,\n",
    "    task.get(\"batchsize\"), \n",
    "    **task.get(\"data_args\", {}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8c893833",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dfdeb8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8a640a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e655b761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d651ef",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1e1c9708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d87d921b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mmt_retrieval.data.datasets.ImageSentenceTripletDataset at 0x7f153b6b1e48>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c31427d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "dfd216e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "05f83631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_config.get(\"num_workers\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "32ec61c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    shuffle=shuffle, \n",
    "    batch_size=batch_size, \n",
    "    sampler=sampler, \n",
    "    batch_sampler=batch_sampler, \n",
    "    num_workers=train_config.get(\"num_workers\", 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "370397a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f159250f7b8>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a15f6fb",
   "metadata": {},
   "source": [
    "### Loss definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2c6fdeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmt_retrieval.losses.losses import BatchHardTripletLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3c464e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task[\"loss\"][\"name\"] == \"triplet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5320fec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task[\"loss\"].get(\"margin\", 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "44944058",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = BatchHardTripletLoss(model=model, margin=task[\"loss\"].get(\"margin\", 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "43974d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tasks = [(dataloader, loss)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b00949b",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2c762170",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_config = train_config[\"dev\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05077e7",
   "metadata": {},
   "source": [
    "#### Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1df703b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmt_retrieval.evaluation.evaluations import EmbeddingImageTextRetrievalEvaluator, CrossEncoderImageTextRetrievalEvaluator, \\\n",
    "    RetrieveRerankImageTextRetrievalEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0a84567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluator(data_config, config, model, split):\n",
    "    split_evaluators = []\n",
    "    for task in config[\"tasks\"]:\n",
    "        load_split = task.get(\"overwrite_split\", split)\n",
    "        split_dataset = DATA_LOADER[task[\"name\"]](get_data(data_config, task[\"name\"]), load_split, task.get(\"tiny\", False),\n",
    "                                                  model.image_dict, return_dict=True, **task.get(\"data_args\", {}))\n",
    "        split_eval_name = task[\"evaluator\"][\"name\"]\n",
    "        file_name = f\"{split}-{task['name']}-{split_eval_name}\"\n",
    "        if split_eval_name == \"embedding_itr\" or split_eval_name == \"itr\":\n",
    "            evaluator = EmbeddingImageTextRetrievalEvaluator(split_dataset[\"images\"], split_dataset[\"captions\"], split_dataset[\"imageid2captions\"], split_dataset.get(\"tags\", None),\n",
    "                                                             name=file_name, show_progress_bar=True,\n",
    "                                                             batched_sim=task.get(\"batched_sim\", 0))\n",
    "        elif split_eval_name == \"ce_itr\" or split_eval_name == \"joint_itr\":\n",
    "            evaluator = CrossEncoderImageTextRetrievalEvaluator(split_dataset[\"images\"], split_dataset[\"captions\"], split_dataset[\"imageid2captions\"], split_dataset.get(\"tags\", None),\n",
    "                                                                name=file_name, batch_size=config[\"batchsize\"], show_progress_bar=True)\n",
    "        elif split_eval_name == \"rr_itr\":\n",
    "            evaluator = RetrieveRerankImageTextRetrievalEvaluator(split_dataset[\"images\"], split_dataset[\"captions\"], split_dataset[\"imageid2captions\"], split_dataset.get(\"tags\", None),\n",
    "                                                                  name=file_name, batch_size=config[\"batchsize\"], show_progress_bar=True,\n",
    "                                                                  retrieve=task.get(\"retrieve\", 10), scoring=task.get(\"scoring\", \"standard\"),\n",
    "                                                                  scoring_factor=task.get(\"scoring_factor\", 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3f8b0427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tasks': [{'name': 'flickr30k',\n",
       "   'tiny': False,\n",
       "   'evaluator': {'name': 'embedding_itr'},\n",
       "   'data_args': {'languages': ['en']}},\n",
       "  {'name': 'flickr30k',\n",
       "   'tiny': True,\n",
       "   'evaluator': {'name': 'ce_itr'},\n",
       "   'data_args': {'captions_per_image': 1, 'languages': ['en']}}],\n",
       " 'batchsize': 512,\n",
       " 'main_score_function': 'mean'}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "689f57c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mscoco_folder': 'SEE_README',\n",
       " 'image_feature_folder': '/home/ec2-user/SageMaker/1_data/flickr30k_features/tiny',\n",
       " 'flickr_split_folder': '/home/ec2-user/SageMaker/1_data/flickr30k_split_folder',\n",
       " 'flickr30k_original_folder': '/home/ec2-user/SageMaker/1_data/flickr30k_entities',\n",
       " 'cc_original_folder': 'SEE_README',\n",
       " 'cc_feature_folder': 'SEE_README',\n",
       " 'multi30k_original_folder': 'SEE_README'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "22b5b668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-08 16:32:27 - Flickr30k Image Feature Split exists. Loading from /home/ec2-user/SageMaker/1_data/flickr30k_split_folder\n",
      "2021-08-08 16:32:27 - Start to load Faster-RCNN detected objects from /home/ec2-user/SageMaker/1_data/flickr30k_split_folder/dev_flickr30k_resnet101_faster_rcnn_genome.tsv\n",
      "2021-08-08 16:32:28 - Flickr30k Image Feature Split exists. Loading from /home/ec2-user/SageMaker/1_data/flickr30k_split_folder\n",
      "2021-08-08 16:32:28 - Start to load Faster-RCNN detected objects from /home/ec2-user/SageMaker/1_data/flickr30k_split_folder/dev_flickr30k_resnet101_faster_rcnn_genome.tsv\n"
     ]
    }
   ],
   "source": [
    "dev_evaluator = get_evaluator(config[\"data\"], dev_config, model, \"dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a87e6aa",
   "metadata": {},
   "source": [
    "##### Deep dive into get_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "16f24607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "data_config, config, model, split = config[\"data\"], dev_config, model, \"dev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1b643dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_evaluators = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "666a7725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'flickr30k',\n",
       "  'tiny': False,\n",
       "  'evaluator': {'name': 'embedding_itr'},\n",
       "  'data_args': {'languages': ['en']}},\n",
       " {'name': 'flickr30k',\n",
       "  'tiny': True,\n",
       "  'evaluator': {'name': 'ce_itr'},\n",
       "  'data_args': {'captions_per_image': 1, 'languages': ['en']}}]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config[\"tasks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9919907e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'flickr30k',\n",
       " 'tiny': False,\n",
       " 'evaluator': {'name': 'embedding_itr'},\n",
       " 'data_args': {'languages': ['en']}}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = config[\"tasks\"][0]\n",
    "task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5248e3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dev'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_split = task.get(\"overwrite_split\", split)\n",
    "load_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7f7e1c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-08 16:39:07 - Flickr30k Image Feature Split exists. Loading from /home/ec2-user/SageMaker/1_data/flickr30k_split_folder\n",
      "2021-08-08 16:39:07 - Start to load Faster-RCNN detected objects from /home/ec2-user/SageMaker/1_data/flickr30k_split_folder/dev_flickr30k_resnet101_faster_rcnn_genome.tsv\n"
     ]
    }
   ],
   "source": [
    "split_dataset = DATA_LOADER[task[\"name\"]](\n",
    "    get_data(data_config, task[\"name\"]), \n",
    "    load_split, \n",
    "    task.get(\"tiny\", False),\n",
    "    model.image_dict, \n",
    "    return_dict=True, \n",
    "    **task.get(\"data_args\", {}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "45178c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['images', 'captions', 'imageid2captions', 'tags'])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7ca3997d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split_dataset[\"images\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8d98d172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2000387055', '6911576445', '4984550402', '2814913390', '3541915243']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset[\"images\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c24ca78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'embedding_itr'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_eval_name = task[\"evaluator\"][\"name\"]\n",
    "split_eval_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0cfd3507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dev-flickr30k-embedding_itr'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = f\"{split}-{task['name']}-{split_eval_name}\"\n",
    "file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "434ae634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_eval_name == \"embedding_itr\" or split_eval_name == \"itr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e0f4d077",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = EmbeddingImageTextRetrievalEvaluator(\n",
    "    split_dataset[\"images\"], \n",
    "    split_dataset[\"captions\"], \n",
    "    split_dataset[\"imageid2captions\"],\n",
    "    split_dataset.get(\"tags\", None),\n",
    "    name=file_name, show_progress_bar=True,\n",
    "    batched_sim=task.get(\"batched_sim\", 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0322966e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2000387055_1',\n",
       " '2000387055_2',\n",
       " '2000387055_3',\n",
       " '2000387055_4',\n",
       " '2000387055_5'}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset[\"imageid2captions\"][\"2000387055\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b9e141a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A girl red and white striped , one-sleeved outfit stands with her arms raised in the air .'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset[\"captions\"][\"2000387055_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3d1f46a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mmt_retrieval.evaluation.evaluations.EmbeddingImageTextRetrievalEvaluator at 0x7f159250f400>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b243f9",
   "metadata": {},
   "source": [
    "**CAPTION EMBEDDINGS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "23b60ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "003100b23168418baccffdf5c18a89b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.12 s, sys: 15 ms, total: 1.13 s\n",
      "Wall time: 304 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "caption_embeddings = model.encode(\n",
    "    sentences=evaluator.captions[:10], \n",
    "    show_progress_bar=True, \n",
    "    batch_size=3, \n",
    "    convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c797b6e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1977,  0.0885, -0.0476,  ...,  0.3186,  0.3787,  1.2237],\n",
       "        [ 0.1109, -0.5627, -0.2971,  ..., -0.1173,  0.3215,  1.0074],\n",
       "        [-0.0926,  0.0981, -0.0644,  ..., -0.3189,  0.5240,  1.6126],\n",
       "        ...,\n",
       "        [ 0.1945, -0.0589, -0.4145,  ...,  0.3135,  0.8455,  0.0203],\n",
       "        [ 0.5727,  0.4543, -0.2724,  ...,  0.0539,  0.4742,  0.3150],\n",
       "        [ 1.2888,  0.1356, -0.4167,  ..., -0.9703,  0.8579,  1.5636]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b766346f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 768])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f1a6f9c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A girl red and white striped , one-sleeved outfit stands with her arms raised in the air .',\n",
       " 'A girl in a red and white outfit is standing inside a display with other holiday items .',\n",
       " 'Social awkward girl looking outside a window onto the street outside .']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.captions[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e2f101",
   "metadata": {},
   "source": [
    "**IMAGE EMBEDDINGS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "6568fdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "6640c6ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2000387055', '6911576445', '4984550402', '2814913390', '3541915243']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.images[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "627d3dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mimages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moutput_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'multimodal_embedding'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcross_product_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mconvert_to_numpy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mis_pretokenized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_workers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Encode multimodal input\n",
       ":param sentences: the sentences\n",
       ":param images: imageids\n",
       ":param batch_size: the batch size used for the computation\n",
       ":param show_progress_bar: Output a progress bar when encoding\n",
       ":param output_value:  Default 'multimodal_embedding', to get multimodal embeddings.\n",
       "Can be set to 'token_embeddings' to get embeddings for each input or set it to 'logits' to get the logits from the classification head.\n",
       ":param cross_product_input: If true, will evaluate the cross-product of the sentences and images.\n",
       "Output ordering will be decided by 'for each image for each sentence', i.e. each image with all sentences first.\n",
       ":param convert_to_numpy: If true, the output is a list of numpy vectors. Else, it is a list of pytorch tensors.\n",
       ":param convert_to_tensor: If true, you get one large tensor as return. Overwrites any setting from convert_to_numpy\n",
       ":param is_pretokenized: If is_pretokenized=True, sentences must be a list of integers, containing the tokenized sentences with each token convert to the respective int.\n",
       ":param device: Which torch.device to use for the computation\n",
       ":param num_workers: Number of background-workers to prepare data. Set to positive number to increase speed\n",
       ":return:\n",
       "   By default, a list of tensors is returned. If convert_to_tensor, a stacked tensor is returned. If convert_to_numpy, a numpy matrix is returned.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/SageMaker/MMT-Retrieval/mmt_retrieval/mmt.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.encode?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "bfcdfe8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1213185795', '202175131', '4898266768', '5737435305', '23016250']"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.image_dict.keys())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "24c54c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.34 s, sys: 37.7 ms, total: 1.38 s\n",
      "Wall time: 361 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "image_embeddings = model.encode(\n",
    "    sentences=evaluator.tags, \n",
    "    #images=evaluator.images[:5], \n",
    "    images=list(model.image_dict.keys())[:10],\n",
    "    show_progress_bar=False, \n",
    "    batch_size=3, \n",
    "    convert_to_tensor=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "bde8f001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4355,  0.3462,  0.3862,  ..., -0.0315, -0.5879,  0.7955],\n",
       "        [ 0.7523, -0.3277, -0.1638,  ...,  0.2090, -0.5337, -0.4010],\n",
       "        [ 0.7111,  0.1779, -0.0382,  ..., -0.2678, -0.3330,  1.2341],\n",
       "        ...,\n",
       "        [ 1.2927,  0.0137, -0.0959,  ...,  0.2119, -0.7002,  0.9523],\n",
       "        [ 1.0059,  0.4814, -0.0499,  ..., -0.1109, -0.3053,  1.8183],\n",
       "        [ 1.3537,  0.1951,  0.1604,  ..., -0.0261, -0.4137,  1.3153]])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "06e3bbf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 768])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e0d6da",
   "metadata": {},
   "source": [
    "#### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1a4b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "optimizer_class = transformers.AdamW\n",
    "optimizer_params={\"lr\": train_config.get(\"lr\", 2e-5), \"eps\": train_config.get(\"eps\", 1e-6)}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
