{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68bea146",
   "metadata": {},
   "source": [
    "# Exploration on MMT Retrieval repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed6bec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HOME_DIR = \"/home/ec2-user/SageMaker\"\n",
    "os.chdir(f\"{HOME_DIR}/MMT-Retrieval/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e239ebbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers>=0.4.1.2\n",
      "  Using cached sentence_transformers-2.0.0-py3-none-any.whl\n",
      "Collecting tqdm>=4.32.1\n",
      "  Downloading tqdm-4.62.0-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 6.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.22.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (2.25.1)\n",
      "Collecting transformers>=4.1.1\n",
      "  Using cached transformers-4.9.1-py3-none-any.whl (2.6 MB)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from -r requirements.txt (line 5)) (1.19.5)\n",
      "Collecting torch>=1.6.0\n",
      "  Using cached torch-1.9.0-cp36-cp36m-manylinux1_x86_64.whl (831.4 MB)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sentence_transformers>=0.4.1.2->-r requirements.txt (line 1)) (1.5.3)\n",
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sentence_transformers>=0.4.1.2->-r requirements.txt (line 1)) (3.4.4)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.10.0-cp36-cp36m-manylinux1_x86_64.whl (22.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 22.1 MB 54.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sentence_transformers>=0.4.1.2->-r requirements.txt (line 1)) (0.24.1)\n",
      "Collecting huggingface-hub\n",
      "  Downloading huggingface_hub-0.0.15-py3-none-any.whl (43 kB)\n",
      "\u001b[K     |████████████████████████████████| 43 kB 257 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.96-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.22.0->-r requirements.txt (line 3)) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.22.0->-r requirements.txt (line 3)) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.22.0->-r requirements.txt (line 3)) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.22.0->-r requirements.txt (line 3)) (1.26.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers>=4.1.1->-r requirements.txt (line 4)) (5.4.1)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Using cached tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers>=4.1.1->-r requirements.txt (line 4)) (0.8)\n",
      "Collecting huggingface-hub\n",
      "  Using cached huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers>=4.1.1->-r requirements.txt (line 4)) (3.0.12)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers>=4.1.1->-r requirements.txt (line 4)) (3.7.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers>=4.1.1->-r requirements.txt (line 4)) (2020.11.13)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers>=4.1.1->-r requirements.txt (line 4)) (20.9)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from huggingface-hub->sentence_transformers>=0.4.1.2->-r requirements.txt (line 1)) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from packaging->transformers>=4.1.1->-r requirements.txt (line 4)) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata->transformers>=4.1.1->-r requirements.txt (line 4)) (3.4.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from nltk->sentence_transformers>=0.4.1.2->-r requirements.txt (line 1)) (1.15.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sacremoses->transformers>=4.1.1->-r requirements.txt (line 4)) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sacremoses->transformers>=4.1.1->-r requirements.txt (line 4)) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from scikit-learn->sentence_transformers>=0.4.1.2->-r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: pillow>=5.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from torchvision->sentence_transformers>=0.4.1.2->-r requirements.txt (line 1)) (8.2.0)\n",
      "Installing collected packages: tqdm, torch, tokenizers, sacremoses, huggingface-hub, transformers, torchvision, sentencepiece, sentence-transformers\n",
      "Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 sentence-transformers-2.0.0 sentencepiece-0.1.96 tokenizers-0.10.3 torch-1.9.0 torchvision-0.10.0 tqdm-4.62.0 transformers-4.9.1\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.2.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1620a739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting anytree\n",
      "  Using cached anytree-2.8.0-py2.py3-none-any.whl (41 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from anytree) (1.15.0)\n",
      "Installing collected packages: anytree\n",
      "Successfully installed anytree-2.8.0\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.2.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install anytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ac545e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmt_retrieval.model.models import OSCAR, UNITER, M3P, ClassificationHead, Pooling\n",
    "from mmt_retrieval import MultimodalTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ad7eae",
   "metadata": {},
   "source": [
    "# Loading pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e00035a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model M3P\n",
    "path_model_m3p = os.path.join(HOME_DIR, \"model_m3p/0_M3P\")\n",
    "pretrained_model = M3P(model_path = path_model_m3p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12c0455",
   "metadata": {},
   "source": [
    "# Run train test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b19c2ec",
   "metadata": {},
   "source": [
    "See application notebook in MMT-Retrieval/examples/applications/Image_Search.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca690e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/MMT-Retrieval\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3115ae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edce219c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.load(open(\"examples/experiments/super_config.yaml\"), Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "122663d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual modifications\n",
    "config[\"model_path\"] = \"david-test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb93b39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "np.random.seed(config[\"seed\"])\n",
    "random.seed(config[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13f917e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model config\n",
    "model_config = config[\"model\"]\n",
    "\n",
    "# Select model M3P for this demo\n",
    "model_config[\"name\"] = \"m3p\"\n",
    "\n",
    "# Creating folders\n",
    "model_folder_name = f\"{model_config['name']}-{datetime.now().strftime('%Y-%m-%d_%H-%M')}\"\n",
    "model_save_path = os.path.join(config[\"model_path\"], model_folder_name)\n",
    "os.makedirs(model_save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "42088e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <TODO: CHANGE HERE> Put the path of our own model\n",
    "model_config[\"model_path\"] = path_model_m3p\n",
    "model_config[\"pretrained_model_path\"] = path_model_m3p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930dce9f",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9c74ad45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Train'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indicate to Train\n",
    "config.get(\"do_train\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1b7ef9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_config):\n",
    "    if \"legacy\" not in model_config[\"name\"]:\n",
    "        if \"oscar\" in model_config[\"name\"]:\n",
    "            embedding_model = OSCAR(model_config[\"pretrained_model_path\"],\n",
    "                                    max_seq_length=model_config.get(\"max_seq_length\", 70),\n",
    "                                    max_image_seq_len=model_config.get(\"max_image_seq_len\", 50))\n",
    "        elif \"m3p\" in model_config[\"name\"]:\n",
    "            embedding_model = M3P(model_config[\"pretrained_model_path\"],\n",
    "                                  max_seq_length=model_config.get(\"max_seq_length\", 128),\n",
    "                                  max_image_seq_len=model_config.get(\"max_image_seq_len\", 50))\n",
    "        elif \"villa\" in model_config[\"name\"] or \"uniter\" in model_config[\"name\"]:\n",
    "            embedding_model = UNITER(model_config[\"pretrained_model_path\"],\n",
    "                                     max_seq_length=model_config.get(\"max_seq_length\", 70),\n",
    "                                     max_image_seq_len=model_config.get(\"max_image_seq_len\", 50))\n",
    "        if model_config.get(\"half_layers\", False):\n",
    "            module_list = torch.nn.ModuleList()\n",
    "            for i, layer in enumerate(embedding_model.auto_model.encoder.layer):\n",
    "                if i % 2 == 0:\n",
    "                    module_list.append(layer)\n",
    "            embedding_model.auto_model.encoder.layer = module_list\n",
    "\n",
    "        class_head = ClassificationHead(2, model_config.get(\"input_key\", \"pooled_cls_token_embeddings\"), 768,\n",
    "                                        model_config.get(\"classifier_type\", \"linear\"),\n",
    "                                        model_config.get(\"scaling_factor\", 2))\n",
    "        pooling_model = Pooling(768,\n",
    "                                       pooling_mode_mean_tokens=model_config.get(\"mean\", True),\n",
    "                                       pooling_mode_cls_token=model_config.get(\"cls\", False),\n",
    "                                       pooling_mode_max_tokens=model_config.get(\"max\", False))\n",
    "        return MultimodalTransformer(modules=[embedding_model, class_head, pooling_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6d3d697d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_path': '/home/ec2-user/SageMaker/model_m3p/0_M3P',\n",
       " 'model_path_cross': 'YOUR_CROSS_ENCODER',\n",
       " 'model_path_embedding': 'YOUR_EMBEDDING_MODEL',\n",
       " 'name': 'm3p',\n",
       " 'pretrained_model_path': '/home/ec2-user/SageMaker/model_m3p/0_M3P',\n",
       " 'max_seq_length': 70,\n",
       " 'max_image_seq_len': 50,\n",
       " 'input_key': 'pooled_cls_token_embeddings',\n",
       " 'classifier_type': 'linear',\n",
       " 'scaling_factor': 1}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b0736906",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = M3P(\n",
    "    model_config[\"pretrained_model_path\"],\n",
    "    max_seq_length=model_config.get(\"max_seq_length\", 128),\n",
    "    max_image_seq_len=model_config.get(\"max_image_seq_len\", 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5836689d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M3P(\n",
       "  (auto_model): TransformerModel(\n",
       "    (position_embeddings): Embedding(514, 768)\n",
       "    (embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "    (layer_norm_emb): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (image_embeddings): BertImageEmbeddings(\n",
       "      (image_embeddings): Linear(in_features=2048, out_features=768, bias=True)\n",
       "      (image_distbution_embeddings): Linear(in_features=1600, out_features=768, bias=True)\n",
       "      (image_location_embeddings): Linear(in_features=5, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (refine_embeddings): AoA_Refiner_Core(\n",
       "      (layers): ModuleList(\n",
       "        (0): AoA_Refiner_Layer(\n",
       "          (self_attn): MultiHeadedDotAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (aoa_layer): Sequential(\n",
       "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (1): GLU(dim=-1)\n",
       "            )\n",
       "            (dropout_aoa): Dropout(p=0.1, inplace=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (feed_forward): TransformerFFN(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (sublayer): ModuleList(\n",
       "            (0): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): AoA_Refiner_Layer(\n",
       "          (self_attn): MultiHeadedDotAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (aoa_layer): Sequential(\n",
       "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (1): GLU(dim=-1)\n",
       "            )\n",
       "            (dropout_aoa): Dropout(p=0.1, inplace=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (feed_forward): TransformerFFN(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (sublayer): ModuleList(\n",
       "            (0): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): AoA_Refiner_Layer(\n",
       "          (self_attn): MultiHeadedDotAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (aoa_layer): Sequential(\n",
       "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (1): GLU(dim=-1)\n",
       "            )\n",
       "            (dropout_aoa): Dropout(p=0.1, inplace=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (feed_forward): TransformerFFN(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (sublayer): ModuleList(\n",
       "            (0): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): AoA_Refiner_Layer(\n",
       "          (self_attn): MultiHeadedDotAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (aoa_layer): Sequential(\n",
       "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (1): GLU(dim=-1)\n",
       "            )\n",
       "            (dropout_aoa): Dropout(p=0.1, inplace=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (feed_forward): TransformerFFN(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (sublayer): ModuleList(\n",
       "            (0): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): AoA_Refiner_Layer(\n",
       "          (self_attn): MultiHeadedDotAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (aoa_layer): Sequential(\n",
       "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (1): GLU(dim=-1)\n",
       "            )\n",
       "            (dropout_aoa): Dropout(p=0.1, inplace=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (feed_forward): TransformerFFN(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (sublayer): ModuleList(\n",
       "            (0): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): AoA_Refiner_Layer(\n",
       "          (self_attn): MultiHeadedDotAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (aoa_layer): Sequential(\n",
       "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (1): GLU(dim=-1)\n",
       "            )\n",
       "            (dropout_aoa): Dropout(p=0.1, inplace=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (feed_forward): TransformerFFN(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (sublayer): ModuleList(\n",
       "            (0): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (cross_alignment): CrossAlignMatrix(\n",
       "      (att_weight_c): Linear(in_features=768, out_features=1, bias=True)\n",
       "      (att_weight_q): Linear(in_features=768, out_features=1, bias=True)\n",
       "      (att_weight_cq): Linear(in_features=768, out_features=1, bias=True)\n",
       "      (align_output): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (attentions): ModuleList(\n",
       "      (0): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (1): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (2): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (3): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (4): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (5): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (6): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (7): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (8): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (9): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (10): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (11): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm1): ModuleList(\n",
       "      (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (ffns): ModuleList(\n",
       "      (0): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (1): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (2): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (3): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (4): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (5): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (6): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (7): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (8): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (9): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (10): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (11): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm2): ModuleList(\n",
       "      (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (layer_norm15): ModuleList(\n",
       "      (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (encoder_attn): ModuleList(\n",
       "      (0): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (1): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (2): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (3): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (4): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (5): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (6): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (7): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (8): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (9): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (10): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (11): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (pooled_layer): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "    (seq_relationship): Linear(in_features=768, out_features=1, bias=True)\n",
       "    (mrfr_dense): Linear(in_features=768, out_features=2048, bias=True)\n",
       "    (transformer_obj): BertPredictionHeadTransform(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6fb58756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-06 19:18:40 - Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "model = build_model(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae6ab96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
