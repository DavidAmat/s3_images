{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fe6e46a",
   "metadata": {},
   "source": [
    "# Exploration on MMT Retrieval repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deb43f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HOME_DIR = \"/home/ec2-user/SageMaker\"\n",
    "os.chdir(f\"{HOME_DIR}/MMT-Retrieval/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63891fad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence_transformers>=0.4.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (2.0.0)\n",
      "Requirement already satisfied: tqdm>=4.32.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (4.62.0)\n",
      "Requirement already satisfied: requests>=2.22.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (2.25.1)\n",
      "Requirement already satisfied: transformers>=4.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (4.9.1)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from -r requirements.txt (line 5)) (1.19.5)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from -r requirements.txt (line 6)) (1.9.0)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sentence_transformers>=0.4.1.2->-r requirements.txt (line 1)) (0.10.0)\n",
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sentence_transformers>=0.4.1.2->-r requirements.txt (line 1)) (3.4.4)\n",
      "Requirement already satisfied: sentencepiece in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sentence_transformers>=0.4.1.2->-r requirements.txt (line 1)) (0.1.96)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sentence_transformers>=0.4.1.2->-r requirements.txt (line 1)) (1.5.3)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sentence_transformers>=0.4.1.2->-r requirements.txt (line 1)) (0.24.1)\n",
      "Requirement already satisfied: huggingface-hub in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sentence_transformers>=0.4.1.2->-r requirements.txt (line 1)) (0.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.22.0->-r requirements.txt (line 3)) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.22.0->-r requirements.txt (line 3)) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.22.0->-r requirements.txt (line 3)) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.22.0->-r requirements.txt (line 3)) (3.0.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers>=4.1.1->-r requirements.txt (line 4)) (5.4.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers>=4.1.1->-r requirements.txt (line 4)) (3.0.12)\n",
      "Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers>=4.1.1->-r requirements.txt (line 4)) (0.0.45)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers>=4.1.1->-r requirements.txt (line 4)) (0.8)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers>=4.1.1->-r requirements.txt (line 4)) (3.7.0)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers>=4.1.1->-r requirements.txt (line 4)) (20.9)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers>=4.1.1->-r requirements.txt (line 4)) (2020.11.13)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers>=4.1.1->-r requirements.txt (line 4)) (0.10.3)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from huggingface-hub->sentence_transformers>=0.4.1.2->-r requirements.txt (line 1)) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from packaging->transformers>=4.1.1->-r requirements.txt (line 4)) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata->transformers>=4.1.1->-r requirements.txt (line 4)) (3.4.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from nltk->sentence_transformers>=0.4.1.2->-r requirements.txt (line 1)) (1.15.0)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sacremoses->transformers>=4.1.1->-r requirements.txt (line 4)) (1.0.1)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sacremoses->transformers>=4.1.1->-r requirements.txt (line 4)) (7.1.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from scikit-learn->sentence_transformers>=0.4.1.2->-r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: pillow>=5.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from torchvision->sentence_transformers>=0.4.1.2->-r requirements.txt (line 1)) (8.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.2.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10780bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: anytree in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.8.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from anytree) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.2.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install anytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77313401",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmt_retrieval.model.models import OSCAR, UNITER, M3P, ClassificationHead, Pooling\n",
    "from mmt_retrieval import MultimodalTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47325dbf",
   "metadata": {},
   "source": [
    "# Loading pre-trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c169660c",
   "metadata": {},
   "source": [
    "## M3P (no execute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3efeb836",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-08 13:50:02 - Lock 139649952230760 acquired on /home/ec2-user/.cache/huggingface/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516c4071d14d47daa3adc49950fd05d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/512 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-08 13:50:03 - Lock 139649952230760 released on /home/ec2-user/.cache/huggingface/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6.lock\n",
      "2021-08-08 13:50:03 - Lock 139649952230760 acquired on /home/ec2-user/.cache/huggingface/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "578167c759f34388a76872c3ea7f0666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-08 13:50:04 - Lock 139649952230760 released on /home/ec2-user/.cache/huggingface/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8.lock\n",
      "2021-08-08 13:50:04 - Lock 139649952230760 acquired on /home/ec2-user/.cache/huggingface/transformers/daeda8d936162ca65fe6dd158ecce1d8cb56c17d89b78ab86be1558eaef1d76a.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "657c73eea5f94bf781e471513447d67b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-08 13:50:05 - Lock 139649952230760 released on /home/ec2-user/.cache/huggingface/transformers/daeda8d936162ca65fe6dd158ecce1d8cb56c17d89b78ab86be1558eaef1d76a.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7.lock\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model M3P\n",
    "path_model_m3p = os.path.join(HOME_DIR, \"model_m3p/0_M3P\")\n",
    "pretrained_model = M3P(model_path = path_model_m3p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab8e7cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[78011, 158, 91714, 35984, 1444]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model.tokenize(\"Pizza con champiñones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b72ab839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[78011, 158, 79, 52960, 24532, 1430]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model.tokenize(\"Pizza con jamón dulche\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676629d6",
   "metadata": {},
   "source": [
    "### Load image features (no execute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "3e706d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmt_retrieval.data.image_embeddings import ImageDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "1fd1bf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagedict = ImageDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "e6d72bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.2 ms, sys: 67 µs, total: 38.2 ms\n",
      "Wall time: 35 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "imagedict.load_obj_tsv(\n",
    "    fname=\"/home/ec2-user/SageMaker/1_data/flickr30k_features/tiny/train_flickr30k_resnet101_faster_rcnn_genome.tsv.1\",\n",
    "    topk=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "df0d4076",
   "metadata": {},
   "outputs": [],
   "source": [
    "value = imagedict[\"1213185795\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "d9401375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('img_id', '1213185795'),\n",
       "             ('img_w', 500),\n",
       "             ('img_h', 333),\n",
       "             ('num_boxes', 39),\n",
       "             ('boxes',\n",
       "              array([[178.77786 ,   0.      , 499.5     , 189.06422 ],\n",
       "                     [286.2556  ,  91.49405 , 409.02844 , 224.29764 ],\n",
       "                     [  0.      ,  17.339499, 211.28693 , 317.61197 ],\n",
       "                     [  6.189648,  34.51717 , 108.51078 , 292.2112  ],\n",
       "                     [319.35147 , 286.56503 , 466.34363 , 332.445   ],\n",
       "                     [323.2654  ,  30.65785 , 477.2827  ,  78.58263 ],\n",
       "                     [263.98987 , 235.54553 , 499.5     , 332.445   ],\n",
       "                     [  0.      ,   0.      , 316.1118  ,  54.290844],\n",
       "                     [203.84828 , 190.30222 , 499.5     , 277.82635 ],\n",
       "                     [229.45714 , 221.64037 , 275.42932 , 300.42758 ],\n",
       "                     [119.19319 ,   0.      , 427.99127 , 332.445   ],\n",
       "                     [ 42.994564,   9.276467, 231.20543 , 332.445   ],\n",
       "                     [100.72046 , 167.77585 , 187.42458 , 332.445   ],\n",
       "                     [455.25516 ,   9.078192, 496.60724 , 101.476776],\n",
       "                     [  0.      , 299.91238 , 100.33168 , 332.445   ],\n",
       "                     [  0.      ,  30.10659 ,  93.24412 , 242.43422 ],\n",
       "                     [298.9117  , 131.05226 , 399.33704 , 199.45631 ],\n",
       "                     [301.5103  ,  40.82225 , 469.55792 , 196.3782  ],\n",
       "                     [163.97351 , 205.35411 , 214.86119 , 322.90744 ],\n",
       "                     [109.22744 , 130.98517 , 195.39713 , 187.38329 ],\n",
       "                     [331.18268 , 103.95835 , 364.7279  , 125.00733 ],\n",
       "                     [172.76146 ,   0.      , 448.96564 ,  70.21708 ],\n",
       "                     [398.46008 , 186.65256 , 441.28268 , 234.32088 ],\n",
       "                     [269.0137  ,  71.28347 , 296.1825  , 116.834984],\n",
       "                     [106.548584,  54.401344, 146.73668 ,  92.224106],\n",
       "                     [107.1162  , 121.05843 , 181.70676 , 194.04994 ],\n",
       "                     [184.49614 ,  37.30108 , 212.90971 ,  80.327484],\n",
       "                     [448.3342  , 197.78416 , 484.78122 , 234.24266 ],\n",
       "                     [ 87.02091 ,  15.206438, 336.498   , 135.91487 ],\n",
       "                     [375.79132 ,  66.140854, 394.2036  ,  86.708595],\n",
       "                     [108.09845 ,  98.417786, 152.9292  , 137.53912 ],\n",
       "                     [130.47333 , 139.71098 , 181.07495 , 176.97809 ],\n",
       "                     [392.9032  ,   0.      , 499.5     , 258.28555 ],\n",
       "                     [329.4981  , 109.35234 , 362.16846 , 138.67793 ],\n",
       "                     [158.03471 , 105.09128 , 189.18188 , 132.92972 ],\n",
       "                     [464.4164  ,   6.426557, 495.07474 ,  23.493181],\n",
       "                     [431.10962 , 187.02502 , 497.26797 , 246.83586 ],\n",
       "                     [115.174675, 182.58179 , 150.28006 , 332.445   ],\n",
       "                     [430.56476 , 227.17807 , 450.7564  , 245.1039  ]], dtype=float32)),\n",
       "             ('features',\n",
       "              array([[4.4259453e-01, 4.5629039e-01, 2.2752611e-01, ..., 6.9394588e-01,\n",
       "                      4.2044764e+00, 0.0000000e+00],\n",
       "                     [1.0395095e+00, 0.0000000e+00, 0.0000000e+00, ..., 4.5075051e-02,\n",
       "                      4.0495762e-01, 0.0000000e+00],\n",
       "                     [2.7406731e+00, 2.4304210e-01, 0.0000000e+00, ..., 4.4396430e-01,\n",
       "                      0.0000000e+00, 2.7462265e-01],\n",
       "                     ...,\n",
       "                     [0.0000000e+00, 5.2186966e+00, 0.0000000e+00, ..., 4.1836292e-02,\n",
       "                      2.7950516e-03, 8.6152625e-01],\n",
       "                     [1.5662067e+00, 5.3293669e-01, 1.5880404e-02, ..., 0.0000000e+00,\n",
       "                      0.0000000e+00, 3.2213101e-01],\n",
       "                     [4.0294129e-02, 7.2604384e+00, 9.3506306e-02, ..., 0.0000000e+00,\n",
       "                      0.0000000e+00, 8.3531260e-01]], dtype=float32))])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagedict[\"1213185795\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "3f8baf77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 4)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coordinates of boxes\n",
    "imagedict[\"1213185795\"][\"boxes\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "3a0e6a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 2048)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# boxes features\n",
    "imagedict[\"1213185795\"][\"features\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "635dd607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(imagedict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8851c0d8",
   "metadata": {},
   "source": [
    "# Run train test (execute)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810e0396",
   "metadata": {},
   "source": [
    "See application notebook in MMT-Retrieval/examples/applications/Image_Search.ipynb \\\n",
    "**See code examples/experiments/run_train_test.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9053f3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/MMT-Retrieval\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5292324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import yaml\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0df3ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.load(open(\"examples/experiments/super_config.yaml\"), Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a07c236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model paths\n",
    "path_model_m3p = os.path.join(HOME_DIR, \"model_m3p/0_M3P\")\n",
    "path_image_feature_folder = \"/home/ec2-user/SageMaker/1_data/flickr30k_features/tiny\"\n",
    "path_flickr30k_original_folder = \"/home/ec2-user/SageMaker/1_data/flickr30k_entities\"\n",
    "path_flickr_split_folder = \"/home/ec2-user/SageMaker/1_data/flickr30k_split_folder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b789e905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######################################\n",
    "\n",
    "#   IMPORTANT!! modify config paths here\n",
    "\n",
    "# ######################################\n",
    "# Manual modifications\n",
    "config[\"model_path\"] = \"david-test\"\n",
    "\n",
    "# model name\n",
    "config[\"model\"][\"name\"] = \"m3p\"\n",
    "\n",
    "# Model path of the pretrained weights\n",
    "config[\"model\"][\"model_path\"] = path_model_m3p\n",
    "config[\"model\"][\"pretrained_model_path\"] = path_model_m3p\n",
    "\n",
    "# Path of the flickr features downloaded from  \n",
    "# https://drive.google.com/uc?export=download&id=11OD_qq7ITBarJwWZfi0bWIRw3HPEaHwE \n",
    "#(source: https://github.com/jnhwkim/ban-vqa/blob/master/tools/download_flickr.sh)\n",
    "config[\"data\"][\"image_feature_folder\"] = path_image_feature_folder\n",
    "config[\"data\"][\"flickr30k_original_folder\"] = path_flickr30k_original_folder\n",
    "config[\"data\"][\"flickr_split_folder\"] = path_flickr_split_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b173fba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "np.random.seed(config[\"seed\"])\n",
    "random.seed(config[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42e506d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model config\n",
    "model_config = config[\"model\"]\n",
    "\n",
    "# Creating folders\n",
    "model_folder_name = f\"{model_config['name']}-{datetime.now().strftime('%Y-%m-%d_%H-%M')}\"\n",
    "model_save_path = os.path.join(config[\"model_path\"], model_folder_name)\n",
    "os.makedirs(model_save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69a4ecae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <TODO: CHANGE HERE> Put the path of our own model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7f499f",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc5788d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Train'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indicate to Train\n",
    "config.get(\"do_train\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee35d471",
   "metadata": {},
   "source": [
    "#### Output\"build_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ec1e74b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_path': '/home/ec2-user/SageMaker/model_m3p/0_M3P',\n",
       " 'model_path_cross': 'YOUR_CROSS_ENCODER',\n",
       " 'model_path_embedding': 'YOUR_EMBEDDING_MODEL',\n",
       " 'name': 'm3p',\n",
       " 'pretrained_model_path': '/home/ec2-user/SageMaker/model_m3p/0_M3P',\n",
       " 'max_seq_length': 70,\n",
       " 'max_image_seq_len': 50,\n",
       " 'input_key': 'pooled_cls_token_embeddings',\n",
       " 'classifier_type': 'linear',\n",
       " 'scaling_factor': 1}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fc167f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_config):\n",
    "    if \"legacy\" not in model_config[\"name\"]:\n",
    "        if \"m3p\" in model_config[\"name\"]:\n",
    "            embedding_model = M3P(model_config[\"pretrained_model_path\"],\n",
    "                                  max_seq_length=model_config.get(\"max_seq_length\", 128),\n",
    "                                  max_image_seq_len=model_config.get(\"max_image_seq_len\", 50))\n",
    "        if model_config.get(\"half_layers\", False):\n",
    "            module_list = torch.nn.ModuleList()\n",
    "            for i, layer in enumerate(embedding_model.auto_model.encoder.layer):\n",
    "                if i % 2 == 0:\n",
    "                    module_list.append(layer)\n",
    "            embedding_model.auto_model.encoder.layer = module_list\n",
    "\n",
    "        class_head = ClassificationHead(2, model_config.get(\"input_key\", \"pooled_cls_token_embeddings\"), 768,\n",
    "                                        model_config.get(\"classifier_type\", \"linear\"),\n",
    "                                        model_config.get(\"scaling_factor\", 2))\n",
    "        pooling_model = Pooling(768,\n",
    "                                       pooling_mode_mean_tokens=model_config.get(\"mean\", True),\n",
    "                                       pooling_mode_cls_token=model_config.get(\"cls\", False),\n",
    "                                       pooling_mode_max_tokens=model_config.get(\"max\", False))\n",
    "        return MultimodalTransformer(modules=[embedding_model, class_head, pooling_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad1f2760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-08 18:53:04 - Use pytorch device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "model = build_model(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7477144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[72625, 9, 21854]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenize(\"Coca-cola\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00d1d05e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[33112, 21049, 11, 158, 41, 991, 113, 33156, 1073]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenize(\"Hamburguesa con queso y patatas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bc2ebf",
   "metadata": {},
   "source": [
    "#### Deep dive \"build_model\" (no execute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff34a4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_config):\n",
    "    if \"legacy\" not in model_config[\"name\"]:\n",
    "        if \"oscar\" in model_config[\"name\"]:\n",
    "            embedding_model = OSCAR(model_config[\"pretrained_model_path\"],\n",
    "                                    max_seq_length=model_config.get(\"max_seq_length\", 70),\n",
    "                                    max_image_seq_len=model_config.get(\"max_image_seq_len\", 50))\n",
    "        elif \"m3p\" in model_config[\"name\"]:\n",
    "            embedding_model = M3P(model_config[\"pretrained_model_path\"],\n",
    "                                  max_seq_length=model_config.get(\"max_seq_length\", 128),\n",
    "                                  max_image_seq_len=model_config.get(\"max_image_seq_len\", 50))\n",
    "        elif \"villa\" in model_config[\"name\"] or \"uniter\" in model_config[\"name\"]:\n",
    "            embedding_model = UNITER(model_config[\"pretrained_model_path\"],\n",
    "                                     max_seq_length=model_config.get(\"max_seq_length\", 70),\n",
    "                                     max_image_seq_len=model_config.get(\"max_image_seq_len\", 50))\n",
    "        if model_config.get(\"half_layers\", False):\n",
    "            module_list = torch.nn.ModuleList()\n",
    "            for i, layer in enumerate(embedding_model.auto_model.encoder.layer):\n",
    "                if i % 2 == 0:\n",
    "                    module_list.append(layer)\n",
    "            embedding_model.auto_model.encoder.layer = module_list\n",
    "\n",
    "        class_head = ClassificationHead(2, model_config.get(\"input_key\", \"pooled_cls_token_embeddings\"), 768,\n",
    "                                        model_config.get(\"classifier_type\", \"linear\"),\n",
    "                                        model_config.get(\"scaling_factor\", 2))\n",
    "        pooling_model = Pooling(768,\n",
    "                                       pooling_mode_mean_tokens=model_config.get(\"mean\", True),\n",
    "                                       pooling_mode_cls_token=model_config.get(\"cls\", False),\n",
    "                                       pooling_mode_max_tokens=model_config.get(\"max\", False))\n",
    "        return MultimodalTransformer(modules=[embedding_model, class_head, pooling_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "373f0943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <TODO: DIG DIVE INTO M3P initialization> (see model/models.py)\n",
    "embedding_model = M3P(\n",
    "    model_config[\"pretrained_model_path\"],\n",
    "    max_seq_length=model_config.get(\"max_seq_length\", 128),\n",
    "    max_image_seq_len=model_config.get(\"max_image_seq_len\", 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "200fec53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M3P(\n",
       "  (auto_model): TransformerModel(\n",
       "    (position_embeddings): Embedding(514, 768)\n",
       "    (embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "    (layer_norm_emb): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (image_embeddings): BertImageEmbeddings(\n",
       "      (image_embeddings): Linear(in_features=2048, out_features=768, bias=True)\n",
       "      (image_distbution_embeddings): Linear(in_features=1600, out_features=768, bias=True)\n",
       "      (image_location_embeddings): Linear(in_features=5, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (refine_embeddings): AoA_Refiner_Core(\n",
       "      (layers): ModuleList(\n",
       "        (0): AoA_Refiner_Layer(\n",
       "          (self_attn): MultiHeadedDotAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (aoa_layer): Sequential(\n",
       "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (1): GLU(dim=-1)\n",
       "            )\n",
       "            (dropout_aoa): Dropout(p=0.1, inplace=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (feed_forward): TransformerFFN(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (sublayer): ModuleList(\n",
       "            (0): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): AoA_Refiner_Layer(\n",
       "          (self_attn): MultiHeadedDotAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (aoa_layer): Sequential(\n",
       "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (1): GLU(dim=-1)\n",
       "            )\n",
       "            (dropout_aoa): Dropout(p=0.1, inplace=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (feed_forward): TransformerFFN(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (sublayer): ModuleList(\n",
       "            (0): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): AoA_Refiner_Layer(\n",
       "          (self_attn): MultiHeadedDotAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (aoa_layer): Sequential(\n",
       "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (1): GLU(dim=-1)\n",
       "            )\n",
       "            (dropout_aoa): Dropout(p=0.1, inplace=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (feed_forward): TransformerFFN(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (sublayer): ModuleList(\n",
       "            (0): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): AoA_Refiner_Layer(\n",
       "          (self_attn): MultiHeadedDotAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (aoa_layer): Sequential(\n",
       "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (1): GLU(dim=-1)\n",
       "            )\n",
       "            (dropout_aoa): Dropout(p=0.1, inplace=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (feed_forward): TransformerFFN(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (sublayer): ModuleList(\n",
       "            (0): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): AoA_Refiner_Layer(\n",
       "          (self_attn): MultiHeadedDotAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (aoa_layer): Sequential(\n",
       "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (1): GLU(dim=-1)\n",
       "            )\n",
       "            (dropout_aoa): Dropout(p=0.1, inplace=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (feed_forward): TransformerFFN(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (sublayer): ModuleList(\n",
       "            (0): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): AoA_Refiner_Layer(\n",
       "          (self_attn): MultiHeadedDotAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (aoa_layer): Sequential(\n",
       "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (1): GLU(dim=-1)\n",
       "            )\n",
       "            (dropout_aoa): Dropout(p=0.1, inplace=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (feed_forward): TransformerFFN(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (sublayer): ModuleList(\n",
       "            (0): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): SublayerConnection(\n",
       "              (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (cross_alignment): CrossAlignMatrix(\n",
       "      (att_weight_c): Linear(in_features=768, out_features=1, bias=True)\n",
       "      (att_weight_q): Linear(in_features=768, out_features=1, bias=True)\n",
       "      (att_weight_cq): Linear(in_features=768, out_features=1, bias=True)\n",
       "      (align_output): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (attentions): ModuleList(\n",
       "      (0): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (1): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (2): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (3): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (4): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (5): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (6): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (7): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (8): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (9): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (10): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (11): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm1): ModuleList(\n",
       "      (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (ffns): ModuleList(\n",
       "      (0): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (1): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (2): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (3): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (4): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (5): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (6): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (7): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (8): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (9): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (10): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (11): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm2): ModuleList(\n",
       "      (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (layer_norm15): ModuleList(\n",
       "      (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (encoder_attn): ModuleList(\n",
       "      (0): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (1): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (2): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (3): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (4): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (5): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (6): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (7): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (8): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (9): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (10): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (11): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (pooled_layer): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "    (seq_relationship): Linear(in_features=768, out_features=1, bias=True)\n",
       "    (mrfr_dense): Linear(in_features=768, out_features=2048, bias=True)\n",
       "    (transformer_obj): BertPredictionHeadTransform(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e64ec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <TODO: DIG DIVE INTO ClassificationHead initialization> (see model/models.py)\n",
    "class_head = ClassificationHead(\n",
    "    2, model_config.get(\"input_key\", \"pooled_cls_token_embeddings\"), 768,\n",
    "    model_config.get(\"classifier_type\", \"linear\"),\n",
    "    model_config.get(\"scaling_factor\", 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "27c5a67a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassificationHead(\n",
       "  (_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "48de59ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pooled_cls_token_embeddings'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config.get(\"input_key\", \"pooled_cls_token_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8e252368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'linear'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config.get(\"classifier_type\", \"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "01940fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config.get(\"scaling_factor\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "556f6df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <TODO: DIG DIVE INTO ClassificationHead initialization> (see model/models.py)\n",
    "pooling_model = Pooling(\n",
    "    768,\n",
    "   pooling_mode_mean_tokens=model_config.get(\"mean\", True),\n",
    "   pooling_mode_cls_token=model_config.get(\"cls\", False),\n",
    "   pooling_mode_max_tokens=model_config.get(\"max\", False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "848cc47b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config.get(\"cls\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "21e18fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config.get(\"mean\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d967bdd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pooling()"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooling_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e5e8b7",
   "metadata": {},
   "source": [
    "## Task in train_config (execute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6156866b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.experiments.process_data import get_sampler, DATA_LOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a48bca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = config[\"training\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ac45d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = train_config[\"tasks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12add662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'flickr30k',\n",
       "  'tiny': False,\n",
       "  'batchsize': 128,\n",
       "  'loss': {'name': 'triplet', 'margin': 0.1},\n",
       "  'data_args': {'jit_loading': False, 'languages': ['en', 'de', 'cs', 'fr']}},\n",
       " {'name': 'flickr30k',\n",
       "  'tiny': False,\n",
       "  'batchsize': 128,\n",
       "  'loss': {'name': 'joint'},\n",
       "  'data_args': {'jit_loading': False, 'languages': ['en', 'de', 'cs', 'fr']}},\n",
       " {'name': 'flickr30k',\n",
       "  'batchsize': 16,\n",
       "  'tiny': False,\n",
       "  'loss': {'name': 'ance', 'margin': 0.1},\n",
       "  'data_args': {'topk': 50, 'negative_examples': 7, 'sim_batchsize': 512}},\n",
       " {'name': 'flickr30k',\n",
       "  'tiny': False,\n",
       "  'batchsize': 128,\n",
       "  'do_hard': True,\n",
       "  'loss': {'name': 'joint'},\n",
       "  'data_args': {'sim_batchsize': 512, 'topk': 50, 'hard_p': 0.05}}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5af2f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = tasks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "291cb2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flickr30k'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task[\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73b5549a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining get_data\n",
    "def get_data(data_config, name):\n",
    "    \"\"\"\n",
    "    for backwards compatibility of old configs\n",
    "    :param data_config:\n",
    "    :param name:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if \"all\" in data_config:\n",
    "        return data_config[\"all\"]\n",
    "    elif name in data_config:\n",
    "        return data_config[name]\n",
    "    else:\n",
    "        return data_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "969903ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flickr30k'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task[\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7161f758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function examples.experiments.process_data.get_flickr30k(config, split, tiny, image_dict, **args)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_LOADER[task[\"name\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de343c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mscoco_folder': 'SEE_README',\n",
       " 'image_feature_folder': '/home/ec2-user/SageMaker/1_data/flickr30k_features/tiny',\n",
       " 'flickr_split_folder': '/home/ec2-user/SageMaker/1_data/flickr30k_split_folder',\n",
       " 'flickr30k_original_folder': '/home/ec2-user/SageMaker/1_data/flickr30k_entities',\n",
       " 'cc_original_folder': 'SEE_README',\n",
       " 'cc_feature_folder': 'SEE_README',\n",
       " 'multi30k_original_folder': 'SEE_README'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8459bc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting get_flickr30k function (filling arguments of get_flickr30k) <ARGUMENTS>\n",
    "config_get = get_data(config[\"data\"], task[\"name\"])\n",
    "split = \"train\"\n",
    "tiny = task.get(\"tiny\", False)\n",
    "image_dict = model.image_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9c1013a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mscoco_folder': 'SEE_README',\n",
       " 'image_feature_folder': '/home/ec2-user/SageMaker/1_data/flickr30k_features/tiny',\n",
       " 'flickr_split_folder': '/home/ec2-user/SageMaker/1_data/flickr30k_split_folder',\n",
       " 'flickr30k_original_folder': '/home/ec2-user/SageMaker/1_data/flickr30k_entities',\n",
       " 'cc_original_folder': 'SEE_README',\n",
       " 'cc_feature_folder': 'SEE_README',\n",
       " 'multi30k_original_folder': 'SEE_README'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_data(config[\"data\"], task[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "372ff611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-08 18:54:23 - Flickr30k Image Feature Split exists. Loading from /home/ec2-user/SageMaker/1_data/flickr30k_split_folder\n",
      "2021-08-08 18:54:23 - Start to load Faster-RCNN detected objects from /home/ec2-user/SageMaker/1_data/flickr30k_split_folder/train_flickr30k_resnet101_faster_rcnn_genome.tsv\n",
      "CPU times: user 6.44 s, sys: 252 ms, total: 6.69 s\n",
      "Wall time: 6.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ENSURE we delete the folder flickr30k_split_folder\n",
    "dataset = DATA_LOADER[task[\"name\"]](\n",
    "    get_data(config[\"data\"], task[\"name\"]), \n",
    "    \"train\", \n",
    "    task.get(\"tiny\", False), \n",
    "    model.image_dict,\n",
    "    joint=task[\"loss\"][\"name\"] == \"joint\", \n",
    "    **task.get(\"data_args\", \n",
    "    {}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8e201c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how this code created a folder named flickr30k_split_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dcdae9fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mmt_retrieval.data.datasets.ImageSentenceTripletDataset at 0x7f511bfa7dd8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cc0f7749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148915"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Caption keys\n",
    "len(dataset.caption_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0c887895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148915"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Captions \n",
    "len(dataset.captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a0f7c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4328445479_1',\n",
       " '4328445479_2',\n",
       " '4328445479_4',\n",
       " '4328445479_5',\n",
       " '4328445479_3']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.caption_keys[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c5071756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4328445479_1 :  Several kids in what seems to be a classroom , working on a problem together or potentially giving a presentation .\n",
      "4328445479_2 :  Two girls and three boys are presenting in front of a dry erase board .\n",
      "4328445479_4 :  Students are in a classroom , reading something off a paper .\n",
      "4328445479_5 :  Five children stand in front of a whiteboard .\n",
      "4328445479_3 :  Five students stand in front of a whiteboard in a classroom .\n"
     ]
    }
   ],
   "source": [
    "for vv in dataset.caption_keys[:5]:\n",
    "    print(vv, \": \", dataset.captions[vv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "550a1a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4328445479_1\n",
      "4328445479\n"
     ]
    }
   ],
   "source": [
    "item = 0\n",
    "pos_caption = dataset.caption_keys[item]\n",
    "print(pos_caption)\n",
    "pos_image = dataset.caption2image[pos_caption]\n",
    "print(pos_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c54263ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4328445479])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label\n",
    "label = torch.LongTensor([int(pos_image)])\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "84d28d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Several kids in what seems to be a classroom , working on a problem together or potentially giving a presentation .',\n",
       " None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Caption of image\n",
    "pos_caption = (dataset.captions[pos_caption], None)\n",
    "pos_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "42c3c576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# No tag\n",
    "tag = dataset.img2tag[pos_image] if dataset.tags else None\n",
    "print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "feac379c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, '4328445479')\n"
     ]
    }
   ],
   "source": [
    "print((tag, pos_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "874168a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[('Several kids in what seems to be a classroom , working on a problem together or potentially giving a presentation .',\n",
       "    None)]],\n",
       " [[(None, '4328445479')]],\n",
       " tensor([4328445479]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing __getitem__\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd39fc4",
   "metadata": {},
   "source": [
    "### Get sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "70803bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function examples.experiments.process_data.get_sampler(name, dataset, batchsize, **args)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "53ca0fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses the class MultidatasetImageSentenceTripletSampler in data > datasets.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "962961ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flickr30k'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task[\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "61c53d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.get(\"batchsize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ded65c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler, batch_sampler, shuffle, batch_size = get_sampler(\n",
    "    task[\"name\"], \n",
    "    dataset,\n",
    "    task.get(\"batchsize\"), \n",
    "    **task.get(\"data_args\", {}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bab3f317",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d8e62f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e79ac6ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "73cb75e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfdac3d",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1a00585b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "817afeab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mmt_retrieval.data.datasets.ImageSentenceTripletDataset at 0x7f511bfa7dd8>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1cae215b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "59cf0a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4e39a165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_config.get(\"num_workers\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b1a47f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    shuffle=shuffle, \n",
    "    batch_size=batch_size, \n",
    "    sampler=sampler, \n",
    "    batch_sampler=batch_sampler, \n",
    "    num_workers=train_config.get(\"num_workers\", 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ce8533fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f5022ea3908>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4887a2",
   "metadata": {},
   "source": [
    "### Loss definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "563f1ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmt_retrieval.losses.losses import BatchHardTripletLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "26e2a2de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task[\"loss\"][\"name\"] == \"triplet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fce5c33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task[\"loss\"].get(\"margin\", 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1776473f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = BatchHardTripletLoss(model=model, margin=task[\"loss\"].get(\"margin\", 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5119620c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tasks = [(dataloader, loss)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd7af61",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ee11585b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_config = train_config[\"dev\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0037e937",
   "metadata": {},
   "source": [
    "#### Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e1810a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmt_retrieval.evaluation.evaluations import EmbeddingImageTextRetrievalEvaluator, CrossEncoderImageTextRetrievalEvaluator, \\\n",
    "    RetrieveRerankImageTextRetrievalEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bcaffbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluator(data_config, config, model, split):\n",
    "    split_evaluators = []\n",
    "    for task in config[\"tasks\"]:\n",
    "        load_split = task.get(\"overwrite_split\", split)\n",
    "        split_dataset = DATA_LOADER[task[\"name\"]](get_data(data_config, task[\"name\"]), load_split, task.get(\"tiny\", False),\n",
    "                                                  model.image_dict, return_dict=True, **task.get(\"data_args\", {}))\n",
    "        split_eval_name = task[\"evaluator\"][\"name\"]\n",
    "        file_name = f\"{split}-{task['name']}-{split_eval_name}\"\n",
    "        if split_eval_name == \"embedding_itr\" or split_eval_name == \"itr\":\n",
    "            evaluator = EmbeddingImageTextRetrievalEvaluator(split_dataset[\"images\"], split_dataset[\"captions\"], split_dataset[\"imageid2captions\"], split_dataset.get(\"tags\", None),\n",
    "                                                             name=file_name, show_progress_bar=True,\n",
    "                                                             batched_sim=task.get(\"batched_sim\", 0))\n",
    "        elif split_eval_name == \"ce_itr\" or split_eval_name == \"joint_itr\":\n",
    "            evaluator = CrossEncoderImageTextRetrievalEvaluator(split_dataset[\"images\"], split_dataset[\"captions\"], split_dataset[\"imageid2captions\"], split_dataset.get(\"tags\", None),\n",
    "                                                                name=file_name, batch_size=config[\"batchsize\"], show_progress_bar=True)\n",
    "        elif split_eval_name == \"rr_itr\":\n",
    "            evaluator = RetrieveRerankImageTextRetrievalEvaluator(split_dataset[\"images\"], split_dataset[\"captions\"], split_dataset[\"imageid2captions\"], split_dataset.get(\"tags\", None),\n",
    "                                                                  name=file_name, batch_size=config[\"batchsize\"], show_progress_bar=True,\n",
    "                                                                  retrieve=task.get(\"retrieve\", 10), scoring=task.get(\"scoring\", \"standard\"),\n",
    "                                                                  scoring_factor=task.get(\"scoring_factor\", 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f05bd9c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tasks': [{'name': 'flickr30k',\n",
       "   'tiny': False,\n",
       "   'evaluator': {'name': 'embedding_itr'},\n",
       "   'data_args': {'languages': ['en']}},\n",
       "  {'name': 'flickr30k',\n",
       "   'tiny': True,\n",
       "   'evaluator': {'name': 'ce_itr'},\n",
       "   'data_args': {'captions_per_image': 1, 'languages': ['en']}}],\n",
       " 'batchsize': 512,\n",
       " 'main_score_function': 'mean'}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "444c9de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mscoco_folder': 'SEE_README',\n",
       " 'image_feature_folder': '/home/ec2-user/SageMaker/1_data/flickr30k_features/tiny',\n",
       " 'flickr_split_folder': '/home/ec2-user/SageMaker/1_data/flickr30k_split_folder',\n",
       " 'flickr30k_original_folder': '/home/ec2-user/SageMaker/1_data/flickr30k_entities',\n",
       " 'cc_original_folder': 'SEE_README',\n",
       " 'cc_feature_folder': 'SEE_README',\n",
       " 'multi30k_original_folder': 'SEE_README'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c2d61399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-08 18:59:13 - Flickr30k Image Feature Split exists. Loading from /home/ec2-user/SageMaker/1_data/flickr30k_split_folder\n",
      "2021-08-08 18:59:13 - Start to load Faster-RCNN detected objects from /home/ec2-user/SageMaker/1_data/flickr30k_split_folder/train_flickr30k_resnet101_faster_rcnn_genome.tsv\n",
      "2021-08-08 18:59:18 - Flickr30k Image Feature Split exists. Loading from /home/ec2-user/SageMaker/1_data/flickr30k_split_folder\n",
      "2021-08-08 18:59:18 - Start to load Faster-RCNN detected objects from /home/ec2-user/SageMaker/1_data/flickr30k_split_folder/train_flickr30k_resnet101_faster_rcnn_genome.tsv\n"
     ]
    }
   ],
   "source": [
    "dev_evaluator = get_evaluator(config[\"data\"], dev_config, model, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2d176c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a6eff0",
   "metadata": {},
   "source": [
    "##### Deep dive into get_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "de714853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "data_config, config, model, split = config[\"data\"], dev_config, model, \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "04513351",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_evaluators = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3aac0590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'flickr30k',\n",
       "  'tiny': False,\n",
       "  'evaluator': {'name': 'embedding_itr'},\n",
       "  'data_args': {'languages': ['en']}},\n",
       " {'name': 'flickr30k',\n",
       "  'tiny': True,\n",
       "  'evaluator': {'name': 'ce_itr'},\n",
       "  'data_args': {'captions_per_image': 1, 'languages': ['en']}}]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config[\"tasks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f8711c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'flickr30k',\n",
       " 'tiny': False,\n",
       " 'evaluator': {'name': 'embedding_itr'},\n",
       " 'data_args': {'languages': ['en']}}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = config[\"tasks\"][0]\n",
    "task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0ab989a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_split = task.get(\"overwrite_split\", split)\n",
    "load_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0a3b3709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mscoco_folder': 'SEE_README',\n",
       " 'image_feature_folder': '/home/ec2-user/SageMaker/1_data/flickr30k_features/tiny',\n",
       " 'flickr_split_folder': '/home/ec2-user/SageMaker/1_data/flickr30k_split_folder',\n",
       " 'flickr30k_original_folder': '/home/ec2-user/SageMaker/1_data/flickr30k_entities',\n",
       " 'cc_original_folder': 'SEE_README',\n",
       " 'cc_feature_folder': 'SEE_README',\n",
       " 'multi30k_original_folder': 'SEE_README'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_data(data_config, task[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "20c4cc15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'languages': ['en']}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.get(\"data_args\", {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cffc792f",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_split = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d95b2b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-08 18:59:34 - Flickr30k Image Feature Split exists. Loading from /home/ec2-user/SageMaker/1_data/flickr30k_split_folder\n",
      "2021-08-08 18:59:34 - Start to load Faster-RCNN detected objects from /home/ec2-user/SageMaker/1_data/flickr30k_split_folder/train_flickr30k_resnet101_faster_rcnn_genome.tsv\n"
     ]
    }
   ],
   "source": [
    "split_dataset = DATA_LOADER[task[\"name\"]](\n",
    "    get_data(data_config, task[\"name\"]), \n",
    "    load_split, \n",
    "    task.get(\"tiny\", False),\n",
    "    model.image_dict, \n",
    "    return_dict=True, \n",
    "    **task.get(\"data_args\", {}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9425bf5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['images', 'captions', 'imageid2captions', 'tags'])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6f5602c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29783"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split_dataset[\"images\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7cb294f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4612733800', '404591376', '22822422', '3289433994', '2472634822']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset[\"images\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "76ec2b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'embedding_itr'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_eval_name = task[\"evaluator\"][\"name\"]\n",
    "split_eval_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bcf05fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train-flickr30k-embedding_itr'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = f\"{split}-{task['name']}-{split_eval_name}\"\n",
    "file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "585aa54e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_eval_name == \"embedding_itr\" or split_eval_name == \"itr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d9f718fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = EmbeddingImageTextRetrievalEvaluator(\n",
    "    split_dataset[\"images\"], \n",
    "    split_dataset[\"captions\"], \n",
    "    split_dataset[\"imageid2captions\"],\n",
    "    split_dataset.get(\"tags\", None),\n",
    "    name=file_name, show_progress_bar=True,\n",
    "    batched_sim=task.get(\"batched_sim\", 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "90492d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29783"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split_dataset[\"images\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "70176d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "di = split_dataset[\"imageid2captions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4fe1a0c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'4328445479_1',\n",
       " '4328445479_2',\n",
       " '4328445479_3',\n",
       " '4328445479_4',\n",
       " '4328445479_5'}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "di[\"4328445479\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "63516099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'4328445479'}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset[\"imageid2captions\"][\"4328445479_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "02748996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Several kids in what seems to be a classroom , working on a problem together or potentially giving a presentation .'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset[\"captions\"][\"4328445479_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8e4a2ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Students are in a classroom , reading something off a paper .'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset[\"captions\"][\"4328445479_4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f6e87b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mmt_retrieval.evaluation.evaluations.EmbeddingImageTextRetrievalEvaluator at 0x7f5124a9f8d0>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "dc91ee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.get(\"main_score_function\", \"normal\") == \"mean\"\n",
    "msf = lambda scores: scores[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "22ade4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally it is wrapped with the SequentialEvaluator class\n",
    "from sentence_transformers.evaluation import SequentialEvaluator\n",
    "split_evaluators = [evaluator]\n",
    "dev_evaluator = SequentialEvaluator(split_evaluators, main_score_function=msf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "48e89e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sentence_transformers.evaluation.SequentialEvaluator.SequentialEvaluator at 0x7f5124a9f390>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c648605f",
   "metadata": {},
   "source": [
    "**CAPTION EMBEDDINGS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "895019d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b8846dac074dbdb26a32b8858ed4c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 810 ms, sys: 7.09 ms, total: 817 ms\n",
      "Wall time: 224 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "caption_embeddings = model.encode(\n",
    "    sentences=evaluator.captions[:10], \n",
    "    show_progress_bar=True, \n",
    "    batch_size=3, \n",
    "    convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6e4f8979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7562,  0.1262,  0.5512,  ..., -0.0147,  1.1224,  0.6292],\n",
       "        [-0.8599, -0.3763,  0.4521,  ..., -0.1440,  0.0118, -0.7298],\n",
       "        [-1.0304, -0.3263,  0.2968,  ..., -0.3083,  0.5256,  0.5384],\n",
       "        ...,\n",
       "        [-0.4071, -0.5545,  0.0050,  ..., -0.4316, -0.2530, -0.4787],\n",
       "        [-0.4062, -0.4642, -0.0715,  ..., -0.0750, -0.2105, -0.4227],\n",
       "        [ 0.5167, -0.3092,  0.3547,  ..., -0.2126, -0.0420, -0.3917]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "bf6019ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 768])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e4bd4996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Several kids in what seems to be a classroom , working on a problem together or potentially giving a presentation .',\n",
       " 'Two girls and three boys are presenting in front of a dry erase board .',\n",
       " 'Five students stand in front of a whiteboard in a classroom .',\n",
       " 'Students are in a classroom , reading something off a paper .',\n",
       " 'Five children stand in front of a whiteboard .']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.captions[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c8d51d",
   "metadata": {},
   "source": [
    "**IMAGE EMBEDDINGS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "92a87260",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b731a330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4328445479', '4361912325', '7036852925', '540503255', '4591344824']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.images[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "31d6e5a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1213185795', '202175131', '4898266768', '5737435305', '23016250']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.image_dict.keys())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0124e789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 937 ms, sys: 0 ns, total: 937 ms\n",
      "Wall time: 251 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "image_embeddings = model.encode(\n",
    "    sentences=evaluator.tags, \n",
    "    #images=evaluator.images[:5], \n",
    "    images=list(model.image_dict.keys())[:10],\n",
    "    show_progress_bar=False, \n",
    "    batch_size=3, \n",
    "    convert_to_tensor=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "eaf36fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4355,  0.3462,  0.3862,  ..., -0.0315, -0.5879,  0.7955],\n",
       "        [ 0.7523, -0.3277, -0.1638,  ...,  0.2090, -0.5337, -0.4010],\n",
       "        [ 0.7111,  0.1779, -0.0382,  ..., -0.2678, -0.3330,  1.2341],\n",
       "        ...,\n",
       "        [ 1.2927,  0.0137, -0.0959,  ...,  0.2119, -0.7002,  0.9523],\n",
       "        [ 1.0059,  0.4814, -0.0499,  ..., -0.1109, -0.3053,  1.8183],\n",
       "        [ 1.3537,  0.1951,  0.1604,  ..., -0.0261, -0.4137,  1.3153]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3aa6d53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 768])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f60e674",
   "metadata": {},
   "source": [
    "##### Cosine similarity of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0196c938",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.util import pytorch_cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a4876746",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_scores = pytorch_cos_sim(image_embeddings, caption_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b4eff6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0251, -0.0448, -0.0634, -0.0090, -0.0274, -0.2069, -0.1470, -0.1726,\n",
       "         -0.2069,  0.0179],\n",
       "        [-0.0533, -0.0960, -0.0917, -0.1056, -0.0514, -0.0543,  0.0043, -0.0180,\n",
       "         -0.0609,  0.0549],\n",
       "        [-0.0354, -0.0900, -0.0730, -0.0534, -0.0291, -0.1378, -0.0231, -0.0629,\n",
       "         -0.0901,  0.0275],\n",
       "        [-0.0904, -0.0544, -0.0817, -0.0955, -0.0322, -0.0858, -0.0256, -0.0384,\n",
       "         -0.0724,  0.0594],\n",
       "        [-0.0546, -0.0904, -0.0872, -0.0522, -0.0448, -0.0861, -0.0441, -0.0692,\n",
       "         -0.0682,  0.0661],\n",
       "        [-0.0285, -0.0057, -0.0162, -0.0089,  0.0241, -0.0915, -0.0457, -0.0464,\n",
       "         -0.0900,  0.0597],\n",
       "        [-0.0204, -0.0469, -0.0381, -0.0329, -0.0140, -0.1505, -0.0813, -0.0935,\n",
       "         -0.1502,  0.0409],\n",
       "        [-0.0662, -0.0669, -0.0772, -0.0305, -0.0598, -0.1120, -0.0631, -0.0857,\n",
       "         -0.1210,  0.0545],\n",
       "        [-0.0299, -0.1095, -0.0692, -0.0423, -0.0313, -0.1224, -0.0529, -0.0873,\n",
       "         -0.1183,  0.0388],\n",
       "        [-0.0381, -0.0365, -0.0590, -0.0365, -0.0188, -0.1228, -0.0484, -0.0898,\n",
       "         -0.1181,  0.0159]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "50183e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "048ba88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mrr_at_k = [10]\n",
    "ndcg_at_k = [10]\n",
    "recall_at_k = [1, 3, 5, 10]\n",
    "map_at_k = [10]\n",
    "mR_ks = [1, 5, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "686091ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_k = max(max(mrr_at_k), max(ndcg_at_k), max(recall_at_k), max(map_at_k))\n",
    "max_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "22ccfcba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(max_k, len(cos_scores[0]) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e421df1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image to Text\n",
    "cos_scores_top_k_values, cos_scores_top_k_idx = \\\n",
    "    torch.topk(\n",
    "        cos_scores, \n",
    "        min(max_k, len(cos_scores[0]) - 1), \n",
    "        dim=1, \n",
    "        largest=True, \n",
    "        sorted=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "60048119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 9])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_scores_top_k_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b7556f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 9, 3, 4, 6, 2, 7, 5],\n",
       "        [5, 9, 7, 6, 4, 0, 2, 8, 1],\n",
       "        [1, 0, 2, 3, 4, 9, 6, 7, 8],\n",
       "        [1, 9, 7, 6, 4, 2, 8, 5, 0],\n",
       "        [5, 9, 8, 3, 4, 0, 6, 7, 2],\n",
       "        [1, 9, 4, 0, 2, 3, 6, 7, 8],\n",
       "        [1, 0, 2, 3, 4, 9, 6, 7, 8],\n",
       "        [1, 0, 9, 3, 4, 6, 7, 2, 5],\n",
       "        [1, 0, 2, 3, 4, 9, 6, 7, 8],\n",
       "        [1, 9, 4, 3, 7, 0, 6, 2, 8]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_scores_top_k_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "6060b72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "i2t_cos_scores_top_k_values = cos_scores_top_k_values.tolist()\n",
    "i2t_cos_scores_top_k_idx = cos_scores_top_k_idx.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c4bf159a",
   "metadata": {},
   "outputs": [],
   "source": [
    "i2t_result_list = [[] for _ in range(0, len(image_embeddings))]\n",
    "t2i_result_list = [[] for _ in range(0, len(caption_embeddings))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "69915e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index of the batch iteration\n",
    "start_idx = 0\n",
    "\n",
    "for query_itr in range(len(i2t_cos_scores_top_k_values)):\n",
    "    for captions_id, score in zip(i2t_cos_scores_top_k_idx[query_itr], i2t_cos_scores_top_k_values[query_itr]):\n",
    "        captions_id = evaluator.captions_ids[captions_id]\n",
    "        i2t_result_list[start_idx+query_itr].append({'captions_id': captions_id, 'score': score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "475661c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'captions_id': '4328445479_2', 'score': -0.044752687215805054},\n",
       "  {'captions_id': '4328445479_1', 'score': -0.025120362639427185},\n",
       "  {'captions_id': '4361912325_5', 'score': 0.01792328804731369},\n",
       "  {'captions_id': '4328445479_4', 'score': -0.008965553715825081},\n",
       "  {'captions_id': '4328445479_5', 'score': -0.02743445336818695},\n",
       "  {'captions_id': '4361912325_2', 'score': -0.14695048332214355},\n",
       "  {'captions_id': '4328445479_3', 'score': -0.06335169821977615},\n",
       "  {'captions_id': '4361912325_3', 'score': -0.17259559035301208},\n",
       "  {'captions_id': '4361912325_1', 'score': -0.20686817169189453}],\n",
       " [{'captions_id': '4361912325_1', 'score': -0.05434556305408478},\n",
       "  {'captions_id': '4361912325_5', 'score': 0.05491766333580017},\n",
       "  {'captions_id': '4361912325_3', 'score': -0.01797068491578102},\n",
       "  {'captions_id': '4361912325_2', 'score': 0.004297451116144657},\n",
       "  {'captions_id': '4328445479_5', 'score': -0.05144333094358444},\n",
       "  {'captions_id': '4328445479_1', 'score': -0.05332810431718826},\n",
       "  {'captions_id': '4328445479_3', 'score': -0.09169891476631165},\n",
       "  {'captions_id': '4361912325_4', 'score': -0.060909803956747055},\n",
       "  {'captions_id': '4328445479_2', 'score': -0.09603209793567657}],\n",
       " [{'captions_id': '4328445479_2', 'score': -0.08996742963790894},\n",
       "  {'captions_id': '4328445479_1', 'score': -0.0354277528822422},\n",
       "  {'captions_id': '4328445479_3', 'score': -0.07304014265537262},\n",
       "  {'captions_id': '4328445479_4', 'score': -0.05340210348367691},\n",
       "  {'captions_id': '4328445479_5', 'score': -0.02912406623363495},\n",
       "  {'captions_id': '4361912325_5', 'score': 0.027540218085050583},\n",
       "  {'captions_id': '4361912325_2', 'score': -0.023086391389369965},\n",
       "  {'captions_id': '4361912325_3', 'score': -0.06294604390859604},\n",
       "  {'captions_id': '4361912325_4', 'score': -0.09014502167701721}],\n",
       " [{'captions_id': '4328445479_2', 'score': -0.05443471670150757},\n",
       "  {'captions_id': '4361912325_5', 'score': 0.059412598609924316},\n",
       "  {'captions_id': '4361912325_3', 'score': -0.038424309343099594},\n",
       "  {'captions_id': '4361912325_2', 'score': -0.02564745396375656},\n",
       "  {'captions_id': '4328445479_5', 'score': -0.032177604734897614},\n",
       "  {'captions_id': '4328445479_3', 'score': -0.08173497766256332},\n",
       "  {'captions_id': '4361912325_4', 'score': -0.07243634015321732},\n",
       "  {'captions_id': '4361912325_1', 'score': -0.08580698072910309},\n",
       "  {'captions_id': '4328445479_1', 'score': -0.09042233228683472}],\n",
       " [{'captions_id': '4361912325_1', 'score': -0.08608822524547577},\n",
       "  {'captions_id': '4361912325_5', 'score': 0.06607525050640106},\n",
       "  {'captions_id': '4361912325_4', 'score': -0.06818602979183197},\n",
       "  {'captions_id': '4328445479_4', 'score': -0.05224914103746414},\n",
       "  {'captions_id': '4328445479_5', 'score': -0.044846877455711365},\n",
       "  {'captions_id': '4328445479_1', 'score': -0.054634999483823776},\n",
       "  {'captions_id': '4361912325_2', 'score': -0.04412854090332985},\n",
       "  {'captions_id': '4361912325_3', 'score': -0.06916888058185577},\n",
       "  {'captions_id': '4328445479_3', 'score': -0.08716890215873718}],\n",
       " [{'captions_id': '4328445479_2', 'score': -0.0057118237018585205},\n",
       "  {'captions_id': '4361912325_5', 'score': 0.05974960699677467},\n",
       "  {'captions_id': '4328445479_5', 'score': 0.02414872497320175},\n",
       "  {'captions_id': '4328445479_1', 'score': -0.02852628193795681},\n",
       "  {'captions_id': '4328445479_3', 'score': -0.016166163608431816},\n",
       "  {'captions_id': '4328445479_4', 'score': -0.008887091651558876},\n",
       "  {'captions_id': '4361912325_2', 'score': -0.04567898437380791},\n",
       "  {'captions_id': '4361912325_3', 'score': -0.04642158001661301},\n",
       "  {'captions_id': '4361912325_4', 'score': -0.0900120735168457}],\n",
       " [{'captions_id': '4328445479_2', 'score': -0.04689928889274597},\n",
       "  {'captions_id': '4328445479_1', 'score': -0.020350446924567223},\n",
       "  {'captions_id': '4328445479_3', 'score': -0.03812890127301216},\n",
       "  {'captions_id': '4328445479_4', 'score': -0.03291188180446625},\n",
       "  {'captions_id': '4328445479_5', 'score': -0.01395779475569725},\n",
       "  {'captions_id': '4361912325_5', 'score': 0.04085890203714371},\n",
       "  {'captions_id': '4361912325_2', 'score': -0.08125896006822586},\n",
       "  {'captions_id': '4361912325_3', 'score': -0.093497134745121},\n",
       "  {'captions_id': '4361912325_4', 'score': -0.15021227300167084}],\n",
       " [{'captions_id': '4328445479_2', 'score': -0.0669100433588028},\n",
       "  {'captions_id': '4328445479_1', 'score': -0.06619690358638763},\n",
       "  {'captions_id': '4361912325_5', 'score': 0.054471779614686966},\n",
       "  {'captions_id': '4328445479_4', 'score': -0.03049532324075699},\n",
       "  {'captions_id': '4328445479_5', 'score': -0.05980107933282852},\n",
       "  {'captions_id': '4361912325_2', 'score': -0.0631435364484787},\n",
       "  {'captions_id': '4361912325_3', 'score': -0.08570495992898941},\n",
       "  {'captions_id': '4328445479_3', 'score': -0.07723115384578705},\n",
       "  {'captions_id': '4361912325_1', 'score': -0.1120351180434227}],\n",
       " [{'captions_id': '4328445479_2', 'score': -0.10949760675430298},\n",
       "  {'captions_id': '4328445479_1', 'score': -0.02992427907884121},\n",
       "  {'captions_id': '4328445479_3', 'score': -0.06920846551656723},\n",
       "  {'captions_id': '4328445479_4', 'score': -0.04231266677379608},\n",
       "  {'captions_id': '4328445479_5', 'score': -0.031330350786447525},\n",
       "  {'captions_id': '4361912325_5', 'score': 0.03883148729801178},\n",
       "  {'captions_id': '4361912325_2', 'score': -0.052888184785842896},\n",
       "  {'captions_id': '4361912325_3', 'score': -0.08726344257593155},\n",
       "  {'captions_id': '4361912325_4', 'score': -0.11834234744310379}],\n",
       " [{'captions_id': '4328445479_2', 'score': -0.0365481823682785},\n",
       "  {'captions_id': '4361912325_5', 'score': 0.015878207981586456},\n",
       "  {'captions_id': '4328445479_5', 'score': -0.018769511952996254},\n",
       "  {'captions_id': '4328445479_4', 'score': -0.03648465499281883},\n",
       "  {'captions_id': '4361912325_3', 'score': -0.08977358788251877},\n",
       "  {'captions_id': '4328445479_1', 'score': -0.03808044642210007},\n",
       "  {'captions_id': '4361912325_2', 'score': -0.048376791179180145},\n",
       "  {'captions_id': '4328445479_3', 'score': -0.05900750309228897},\n",
       "  {'captions_id': '4361912325_4', 'score': -0.11805883795022964}]]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for each image, it computes the cosine similarity score with the rest of images in the batch\n",
    "i2t_result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7679393e",
   "metadata": {},
   "source": [
    "#### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "410edafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "29d25ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_class = transformers.AdamW\n",
    "optimizer_params={\"lr\": train_config.get(\"lr\", 2e-5), \"eps\": train_config.get(\"eps\", 1e-6)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "0a5f24c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sentence_transformers.evaluation.SequentialEvaluator.SequentialEvaluator at 0x7fe76d5d05f8>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c50dfd7",
   "metadata": {},
   "source": [
    "### Model fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c4b059",
   "metadata": {},
   "source": [
    "#### Arguments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "25b1b5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_tasks = training_tasks = [(dataloader, loss)]\n",
    "# evaluator = dev_evaluator (sequential evaluator from )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "4ba8b5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_objectives=training_tasks\n",
    "optimizer_model=model\n",
    "evaluator=dev_evaluator\n",
    "epochs=train_config.get(\"epochs\", 1)\n",
    "steps_per_epoch=train_config.get(\"steps_per_epoch\", None)\n",
    "gradient_accumulation=train_config.get(\"gradient_accumulation\", 1)\n",
    "evaluation_steps=train_config.get(\"evaluation_steps\", 1000)\n",
    "warmup_steps=train_config.get(\"warmup_steps\", 0.1)\n",
    "output_path=model_save_path\n",
    "use_amp=train_config.get(\"use_amp\", True)\n",
    "optimizer_params=optimizer_params\n",
    "optimizer_class=optimizer_class\n",
    "load_optimizer_scheduler_path=train_config.get(\"load_optimizer_scheduler_path\", None)\n",
    "save_latest_model=train_config.get(\"save_latest_model\", False)\n",
    "weight_decay=train_config.get(\"weight_decay\", 0.01)\n",
    "max_grad_norm=train_config.get(\"max_grad_norm\", 1)\n",
    "use_wandb=False\n",
    "logging_steps=config.get(\"wandb\", {}).get(\"logging_steps\", 0)\n",
    "dataset_callback=None\n",
    "objectives_sequence=train_config.get(\"task_sequence\", None)\n",
    "\n",
    "# default\n",
    "output_path_ignore_not_empty = False\n",
    "scheduler = 'WarmupLinear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "5f57f578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n",
      "1\n",
      "5000\n",
      "0.0\n",
      "david-test/m3p-2021-08-08_18-17\n"
     ]
    }
   ],
   "source": [
    "print(steps_per_epoch)\n",
    "print(gradient_accumulation)\n",
    "print(evaluation_steps)\n",
    "print(warmup_steps)\n",
    "print(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "7a6f0926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "{'lr': 2e-05, 'eps': 1e-06}\n",
      "<class 'transformers.optimization.AdamW'>\n",
      "None\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(use_amp)\n",
    "print(optimizer_params)\n",
    "print(optimizer_class)\n",
    "print(load_optimizer_scheduler_path)\n",
    "print(save_latest_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e3576532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n",
      "1\n",
      "False\n",
      "0\n",
      "None\n",
      "[0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(weight_decay)\n",
    "print(max_grad_norm)\n",
    "print(use_wandb)\n",
    "print(logging_steps)\n",
    "print(dataset_callback)\n",
    "print(objectives_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f735e131",
   "metadata": {},
   "source": [
    "#### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f1307a59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultimodalTransformer(\n",
       "  (0): M3P(\n",
       "    (auto_model): TransformerModel(\n",
       "      (position_embeddings): Embedding(514, 768)\n",
       "      (embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (layer_norm_emb): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (image_embeddings): BertImageEmbeddings(\n",
       "        (image_embeddings): Linear(in_features=2048, out_features=768, bias=True)\n",
       "        (image_distbution_embeddings): Linear(in_features=1600, out_features=768, bias=True)\n",
       "        (image_location_embeddings): Linear(in_features=5, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (refine_embeddings): AoA_Refiner_Core(\n",
       "        (layers): ModuleList(\n",
       "          (0): AoA_Refiner_Layer(\n",
       "            (self_attn): MultiHeadedDotAttention(\n",
       "              (linears): ModuleList(\n",
       "                (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (aoa_layer): Sequential(\n",
       "                (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (1): GLU(dim=-1)\n",
       "              )\n",
       "              (dropout_aoa): Dropout(p=0.1, inplace=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (feed_forward): TransformerFFN(\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (sublayer): ModuleList(\n",
       "              (0): SublayerConnection(\n",
       "                (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): SublayerConnection(\n",
       "                (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): AoA_Refiner_Layer(\n",
       "            (self_attn): MultiHeadedDotAttention(\n",
       "              (linears): ModuleList(\n",
       "                (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (aoa_layer): Sequential(\n",
       "                (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (1): GLU(dim=-1)\n",
       "              )\n",
       "              (dropout_aoa): Dropout(p=0.1, inplace=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (feed_forward): TransformerFFN(\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (sublayer): ModuleList(\n",
       "              (0): SublayerConnection(\n",
       "                (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): SublayerConnection(\n",
       "                (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): AoA_Refiner_Layer(\n",
       "            (self_attn): MultiHeadedDotAttention(\n",
       "              (linears): ModuleList(\n",
       "                (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (aoa_layer): Sequential(\n",
       "                (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (1): GLU(dim=-1)\n",
       "              )\n",
       "              (dropout_aoa): Dropout(p=0.1, inplace=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (feed_forward): TransformerFFN(\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (sublayer): ModuleList(\n",
       "              (0): SublayerConnection(\n",
       "                (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): SublayerConnection(\n",
       "                (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): AoA_Refiner_Layer(\n",
       "            (self_attn): MultiHeadedDotAttention(\n",
       "              (linears): ModuleList(\n",
       "                (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (aoa_layer): Sequential(\n",
       "                (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (1): GLU(dim=-1)\n",
       "              )\n",
       "              (dropout_aoa): Dropout(p=0.1, inplace=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (feed_forward): TransformerFFN(\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (sublayer): ModuleList(\n",
       "              (0): SublayerConnection(\n",
       "                (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): SublayerConnection(\n",
       "                (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (4): AoA_Refiner_Layer(\n",
       "            (self_attn): MultiHeadedDotAttention(\n",
       "              (linears): ModuleList(\n",
       "                (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (aoa_layer): Sequential(\n",
       "                (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (1): GLU(dim=-1)\n",
       "              )\n",
       "              (dropout_aoa): Dropout(p=0.1, inplace=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (feed_forward): TransformerFFN(\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (sublayer): ModuleList(\n",
       "              (0): SublayerConnection(\n",
       "                (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): SublayerConnection(\n",
       "                (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (5): AoA_Refiner_Layer(\n",
       "            (self_attn): MultiHeadedDotAttention(\n",
       "              (linears): ModuleList(\n",
       "                (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (aoa_layer): Sequential(\n",
       "                (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (1): GLU(dim=-1)\n",
       "              )\n",
       "              (dropout_aoa): Dropout(p=0.1, inplace=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (feed_forward): TransformerFFN(\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (sublayer): ModuleList(\n",
       "              (0): SublayerConnection(\n",
       "                (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): SublayerConnection(\n",
       "                (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (cross_alignment): CrossAlignMatrix(\n",
       "        (att_weight_c): Linear(in_features=768, out_features=1, bias=True)\n",
       "        (att_weight_q): Linear(in_features=768, out_features=1, bias=True)\n",
       "        (att_weight_cq): Linear(in_features=768, out_features=1, bias=True)\n",
       "        (align_output): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (attentions): ModuleList(\n",
       "        (0): MultiHeadAttention(\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (1): MultiHeadAttention(\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (2): MultiHeadAttention(\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (3): MultiHeadAttention(\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (4): MultiHeadAttention(\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (5): MultiHeadAttention(\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (6): MultiHeadAttention(\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (7): MultiHeadAttention(\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (8): MultiHeadAttention(\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (9): MultiHeadAttention(\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (10): MultiHeadAttention(\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (11): MultiHeadAttention(\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm1): ModuleList(\n",
       "        (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (ffns): ModuleList(\n",
       "        (0): TransformerFFN(\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (1): TransformerFFN(\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (2): TransformerFFN(\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (3): TransformerFFN(\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (4): TransformerFFN(\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (5): TransformerFFN(\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (6): TransformerFFN(\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (7): TransformerFFN(\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (8): TransformerFFN(\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (9): TransformerFFN(\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (10): TransformerFFN(\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (11): TransformerFFN(\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm2): ModuleList(\n",
       "        (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (layer_norm15): ModuleList(\n",
       "        (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (encoder_attn): ModuleList(\n",
       "        (0): MultiHeadAttention(\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (1): MultiHeadAttention(\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (2): MultiHeadAttention(\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (3): MultiHeadAttention(\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (4): MultiHeadAttention(\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (5): MultiHeadAttention(\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (6): MultiHeadAttention(\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (7): MultiHeadAttention(\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (8): MultiHeadAttention(\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (9): MultiHeadAttention(\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (10): MultiHeadAttention(\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (11): MultiHeadAttention(\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (pooled_layer): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "      (seq_relationship): Linear(in_features=768, out_features=1, bias=True)\n",
       "      (mrfr_dense): Linear(in_features=768, out_features=2048, bias=True)\n",
       "      (transformer_obj): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): ClassificationHead(\n",
       "    (_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       "  (2): Pooling()\n",
       ")"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if use_amp:\n",
    "    from torch.cuda.amp import autocast\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "model.use_wandb = use_wandb\n",
    "model.to(model._target_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a4864c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if output_path is not None:\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    if not output_path_ignore_not_empty and len(os.listdir(output_path)) > 0:\n",
    "        raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(\n",
    "            output_path))\n",
    "    if save_latest_model:\n",
    "        os.makedirs(os.path.join(output_path, \"latest_checkpoint\"), exist_ok=True)\n",
    "\n",
    "dataloaders = [dataloader for dataloader, _ in train_objectives]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "470fecc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'david-test/m3p-2021-08-08_18-17/latest_checkpoint'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(output_path, \"latest_checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "9d5aeaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use smart batching\n",
    "for dataloader in dataloaders:\n",
    "    dataloader.collate_fn = model.smart_batching_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "4dd7e358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile losses to the desired device\n",
    "loss_models = [loss for _, loss in train_objectives]\n",
    "device = model._target_device\n",
    "\n",
    "for loss_model in loss_models:\n",
    "    loss_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "5e3e0ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_score = -9999999\n",
    "\n",
    "if steps_per_epoch is None or steps_per_epoch == 0:\n",
    "    steps_per_epoch = min([len(dataloader) for dataloader in dataloaders])\n",
    "steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "774d575b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_steps = int(steps_per_epoch * epochs)\n",
    "param_optimizer = list(optimizer_model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "fa5861d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([514, 768])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer_grouped_parameters[0][\"params\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "20addca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([250002, 768])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer_grouped_parameters[0][\"params\"][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "8c960852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n",
      "['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n"
     ]
    }
   ],
   "source": [
    "print(num_train_steps)\n",
    "print(no_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "9fd67342",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizer_class(optimizer_grouped_parameters, **optimizer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "31e0f7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(warmup_steps, float):\n",
    "    warmup_steps = int(warmup_steps*num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "449397dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = model._get_scheduler(optimizer, scheduler=scheduler, warmup_steps=warmup_steps, t_total=num_train_steps)\n",
    "model._load_optimizer_scheduler(load_optimizer_scheduler_path, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "05d6cc10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torch.utils.data.dataloader._SingleProcessDataLoaderIter at 0x7fe74db11f28>]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_step = 0\n",
    "data_iterators = [iter(dataloader) for dataloader in dataloaders]\n",
    "data_iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "3e1b039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train objectives tuple( dataloader, loss)\n",
    "num_train_objectives = len(train_objectives)\n",
    "if objectives_sequence is None:\n",
    "    objectives_sequence = range(num_train_objectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "28f7f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_loss_scaling = [len([j for j in objectives_sequence if j==i]) for i in range(num_train_objectives)]\n",
    "skip_scheduler = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "5c378f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_optimizer_scheduler_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458e428d",
   "metadata": {},
   "source": [
    "#### Starting epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "d5f7b603",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "start_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "ca2a0608",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_losses = [torch.tensor(0.0).to(model._target_device) for _ in range(len(loss_models))]\n",
    "logging_losses_scalar = [0.0]*len(loss_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "ae0ce5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0.)]\n",
      "[0.0]\n"
     ]
    }
   ],
   "source": [
    "print(tr_losses)\n",
    "print(logging_losses_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "c06cf43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  for epoch in trange(start_epoch, epochs, desc=\"Epoch\"):\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "fc3aeb4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_steps = start_step\n",
    "training_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "7f8d3743",
   "metadata": {},
   "outputs": [],
   "source": [
    "for loss_model in loss_models:\n",
    "    loss_model.zero_grad()\n",
    "    loss_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b1360311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "c6fdd001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objectives_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "d421da78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = objectives_sequence[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "c329cb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_model = loss_models[train_idx]\n",
    "data_iterator = data_iterators[train_idx]\n",
    "tr_loss = tr_losses[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "f5cdbdf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mmt_retrieval.losses.losses.BatchHardTripletLoss"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(loss_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "65acbd5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader._SingleProcessDataLoaderIter"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "f090dc79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "b706e28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "8ed5823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "0bc31e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader._SingleProcessDataLoaderIter at 0x7fe74db11f28>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "a910ffd6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'5293561334'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-191-ae678a217556>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/SageMaker/MMT-Retrieval/mmt_retrieval/mmt.py\u001b[0m in \u001b[0;36msmart_batching_collate\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    208\u001b[0m                             \u001b[0mmax_seq_lens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_seq_lens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_text_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m                             \u001b[0mimage_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m                             \u001b[0mmax_seq_lens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_seq_lens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                         \u001b[0mpaired_blocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/MMT-Retrieval/mmt_retrieval/data/image_dict.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '5293561334'"
     ]
    }
   ],
   "source": [
    "data = next(data_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94bc481",
   "metadata": {},
   "source": [
    "#### Collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "04155509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we call model.encode we are triggering smart_batching_collate_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b1a6c459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_batching_collate(self, batch):\n",
    "        \"\"\"\n",
    "        Transforms a batch from a SmartBatchingDataset to a batch of tensors for the model\n",
    "        Here, batch is a list of tuples: [(tokens, label), ...]\n",
    "\n",
    "        :param batch:\n",
    "            a batch from a SmartBatchingDataset\n",
    "        :return:\n",
    "            a batch of tensors for the model\n",
    "        \"\"\"\n",
    "        labels = []\n",
    "        num_blocks = len(batch[0]) - 1\n",
    "        num_pairs = [len(batch[0][i]) for i in range(num_blocks)]\n",
    "        paired_blocks = [[[] for _ in range(num_pairs[i])] for i in range(num_blocks)]\n",
    "        max_seq_lens = [[[0, 0] for _ in range(num_pairs[i])] for i in range(num_blocks)]\n",
    "        for item in batch:\n",
    "            blocks, label = item[:-1], item[-1]\n",
    "            for i, block in enumerate(blocks):\n",
    "                for j, pair in enumerate(block):\n",
    "                    for p in pair:\n",
    "                        tokens = None\n",
    "                        image_features = None\n",
    "                        if p[0] is not None:\n",
    "                            tokens = self.tokenize(p[0])\n",
    "                            max_seq_lens[i][j][0] = max(max_seq_lens[i][j][0], self._text_length(tokens))\n",
    "                        if p[1] is not None:\n",
    "                            image_features = self.image_dict[p[1]]\n",
    "                            max_seq_lens[i][j][1] = max(max_seq_lens[i][j][1], len(image_features['features']))\n",
    "                        paired_blocks[i][j].append((tokens, image_features))\n",
    "            if isinstance(label, list):\n",
    "                labels.extend(label)\n",
    "            else:\n",
    "                labels.append(label)\n",
    "\n",
    "        features = []\n",
    "        for i in range(num_blocks):\n",
    "            for j in range(num_pairs[i]):\n",
    "                feature_lists = {}\n",
    "                for pair in paired_blocks[i][j]:\n",
    "                    pair_features = self.get_features(pair[0], pair[1], max_seq_lens[i][j][0], max_seq_lens[i][j][1])\n",
    "\n",
    "                    for feature_name in pair_features:\n",
    "                        if feature_name not in feature_lists:\n",
    "                            feature_lists[feature_name] = []\n",
    "\n",
    "                        feature_lists[feature_name].append(pair_features[feature_name])\n",
    "                for feature_name in feature_lists:\n",
    "                    feature_lists[feature_name] = torch.cat(feature_lists[feature_name])\n",
    "                features.append(feature_lists)\n",
    "\n",
    "        return {'features': features, 'labels': torch.stack(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7912960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = model.tokenize(\"Pizza con jamón dulche\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "af6a64da",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"4328445479\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9421845b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [(token, label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2bc95881",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "num_blocks = len(batch[0]) - 1\n",
    "num_pairs = [len(batch[0][i]) for i in range(num_blocks)]\n",
    "paired_blocks = [[[] for _ in range(num_pairs[i])] for i in range(num_blocks)]\n",
    "max_seq_lens = [[[0, 0] for _ in range(num_pairs[i])] for i in range(num_blocks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9b1c02d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-ee83e904ce27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[0mimage_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "for item in batch:\n",
    "    blocks, label = item[:-1], item[-1]\n",
    "    for i, block in enumerate(blocks):\n",
    "        for j, pair in enumerate(block):\n",
    "            for p in pair:\n",
    "                tokens = None\n",
    "                image_features = None\n",
    "                if p[0] is not None:\n",
    "                    tokens = self.tokenize(p[0])\n",
    "                    max_seq_lens[i][j][0] = max(max_seq_lens[i][j][0], self._text_length(tokens))\n",
    "                if p[1] is not None:\n",
    "                    image_features = self.image_dict[p[1]]\n",
    "                    max_seq_lens[i][j][1] = max(max_seq_lens[i][j][1], len(image_features['features']))\n",
    "                paired_blocks[i][j].append((tokens, image_features))\n",
    "    if isinstance(label, list):\n",
    "        labels.extend(label)\n",
    "    else:\n",
    "        labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "11402a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78011"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df0f920",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
